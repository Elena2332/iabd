{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ultralytics.solutions.solutions import BaseSolution\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import time \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.digitalocean.com/community/tutorials/what-is-new-with-yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 1 \n",
    "Vamos a usar el modleo YOLOv11 para detectar objetos en una imagen (en este caso vamos a aplicar el modelo directamente sin entrenarlo).\n",
    "Vamos a aplicarlo en las imagenes deteccion_objetos.jpg y bus.jpg\n",
    "\n",
    "Partiendo de la imagen deteccion_objetos.jpg y bus.jpg.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_imgs = 'recursos/5_5/'\n",
    "imgs_arr = [ruta_imgs+\"deteccion_objetos.jpg\",ruta_imgs+\"bus.jpg\"]\n",
    "# cargar modelo yolo11\n",
    "model = YOLO(\"recursos/5_5/yolo/yolo11n.pt\")   \n",
    "\n",
    "results = model(imgs_arr)\n",
    "for res in results:\n",
    "    res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a repetir el ejercicio pero ahora vamos a modificar la salida para que quede de la siguiente forma:\n",
    "\n",
    "![Imagen con la el resultado que se quiere obtener 1](recursos/5_5/deteccion_objetos_solucion.png)\n",
    "\n",
    "![Imagen con la el resultado que se quiere obtener 1](recursos/5_5/bus_solucion.png)\n",
    "\n",
    "Además sólo quiero que detecte los objetos cuya probabilidad sea mayor al 50%.\n",
    "\n",
    "Ten en cuenta que al aplicar el modelo a una imagen devuelve diferentes datos:\n",
    " * Cajas:\n",
    "    * Clase (cls)\n",
    "    * Probabilidad (score)\n",
    "    * Coordenadas de la caja xyxy\n",
    "\n",
    "Para representar la imagen y añadir rectángulos, texto... puedes usar Image, ImageDraw, ImageFont de PIL. Dibujas la imagen con .Draw de (ImageDraw) con .rectangle y .text puedes ir añadiendo texto y rectángulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = results[0].names\n",
    "colores = ['cyan','orange','pink','green','yellow','red','blue','white']\n",
    "guia = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,res in enumerate(results):    \n",
    "    boxes = res.boxes  # Boxes object for bounding box outputs\n",
    "    txt = ''\n",
    "    filtrados = []\n",
    "    for j,(cls,prob,coords) in enumerate(zip(boxes.cls,boxes.conf,boxes.xyxy)):\n",
    "        if prob > 0.5:      # probabilidad mayor del 50%            \n",
    "            # datos prediccion\n",
    "            filtrados.append([cls,prob,coords])\n",
    "            txt += 'Recuadro {}--> Clase: {},\\t Score: {},\\t Caja: {}\\n'.format(j, clases[int(cls)], prob, coords)\n",
    "            if int(cls) not in guia.keys(): \n",
    "                print(len(guia.keys()))\n",
    "                guia[int(cls)] = colores[len(guia.keys())]\n",
    "    # imagen\n",
    "    img = Image.open(imgs_arr[i])\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(\"usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf\", size=30)\n",
    "\n",
    "    for i,caso in enumerate(filtrados):\n",
    "        coords = caso[2]\n",
    "        color = guia[int(caso[0])]\n",
    "        draw.rectangle([coords[0], coords[1], coords[2], coords[3]], outline=color, width=5)\n",
    "        draw.rectangle([coords[0], coords[1]-30, coords[0]+2 + 40, coords[1]],fill=\"black\")\n",
    "        draw.rectangle([coords[0]+2, coords[1]-28, coords[0] + 40, coords[1]-2],fill=color)\n",
    "        draw.text((coords[0]+8, coords[1]-32), str(i), fill=\"black\",font=font)\n",
    "\n",
    "    # mostrar resultados\n",
    "    plt.figure(figsize=(13,7))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuleta atributos\n",
    "\"\"\" for res in results:    \n",
    "    boxes = res.boxes          # Boxes object for bounding box outputs\n",
    "    masks = res.masks          # Masks object for segmentation masks outputs\n",
    "    keypoints = res.keypoints          # Keypoints object for pose outputs\n",
    "    probs = res.probs          # Probs object for classification outputs\n",
    "    obb = res.obb            # Oriented boxes object for OBB outputs\n",
    "    res.show()              # display to screen\n",
    "    res.save(filename=\"result.jpg\")          # save to disk\n",
    "    print(boxes.cls)\n",
    "    print(boxes.conf) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 3\n",
    "YOLOv11 también sirve para segmentación, aplica segmentación a las dos imágenes con las que estamos trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentation  \n",
    "model = YOLO('recursos/5_5/yolo/yolo11n-seg.pt')  \n",
    "results = model(imgs_arr)  \n",
    "for res in results:\n",
    "    res.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 4\n",
    "De manera similar, podemos hacer la estimación de pose. Hazlo con las imágenes con las que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('recursos/5_5/yolo/yolo11n-pose.pt')  \n",
    "results = model(imgs_arr)  \n",
    "for res in results:\n",
    "    res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz lo que se hace en el siguiente video:\n",
    "\n",
    "https://www.youtube.com/watch?v=_YbEna8UNcU\n",
    "\n",
    "Tienes parte del código en:\n",
    "\n",
    "https://www.albertcoronado.com/2024/11/12/tutorial-yolo-11/\n",
    "\n",
    "Para la parte fine tuning tienes que entrenar el modelo para que distina matrículas de coche, para entrenar el modelo utiliza el dataset que puedes descargar de:\n",
    "\n",
    "https://universe.roboflow.com/parag-parmar-3qpin/car_license_plate-ypwmk/dataset/1\n",
    "\n",
    "NOTA: HE CONSEGUIDO QUE FUNCIONE BIEN CV2 PERO ABRIENDO PRIMERO UNA IMAGEN:\n",
    "```\n",
    "img = cv2.imread('Datos/image3.jpg')\n",
    "cv2.imshow(\"SAPA\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camara\n",
    "model = YOLO(\"recursos/5_5/yolo/yolo11x.pt\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.7\n",
    "color = (0, 250, 0)\n",
    "thickness = 2\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model( img, stream=True )\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "            confidence = math.ceil((box.conf[0]*100))\n",
    "            \n",
    "            cv2.rectangle(img, ( x1, y1 ), ( x2, y2 ), color, 1)\n",
    "            cv2.putText(img, r.names[ int( box.cls[0] ) ]+\" \"+str(confidence)+\"%\", [ x1+4, y1+25 ], font, fontScale, color, thickness)\n",
    "\n",
    "    cv2.imshow('Webcam', img)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train epochs=20 batch=10 model='yolo11s' data='/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/recursos/datasets/car_license_plate.v1i.yolov11/data.yaml' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir\n",
    "!yolo predict model='yolo11s' source='/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/recursos/5_5/matriculas.jpeg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 6\n",
    "Repite el fine tuning del apartado anterior pero ahora en vez de como un comando en la terminal con código de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rutas\n",
    "model_matriculas = YOLO('yolo11s')\n",
    "data_yaml='/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/recursos/datasets/car_license_plate.v1i.yolov11/data.yaml' \n",
    "\n",
    "# entrenar\n",
    "model_matriculas.train(data = data_yaml,\n",
    "            epochs= 25, \n",
    "            batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir\n",
    "results = model_matriculas.predict('./recursos/5_5/matriculas.jpeg', conf=0.25)\n",
    "for res in results:\n",
    "    res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_matriculas.save('./recursos/5_5/yolo/yolo_matriculas.pt')    # guardar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 7\n",
    "Vamos a contar los pasajeros de un autobus.\n",
    "\n",
    "Sigue las instrucciones del siguiente video:\n",
    "\n",
    "https://www.youtube.com/watch?v=hTI0Aj__SLE\n",
    "\n",
    "ELIMINA LOS COMENTARIOS QUE HAY EN EL CÓDIGO Y COMENTA EL CÓDIGO CON TUS PROPIAS PALABRAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.solutions.solutions import BaseSolution\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class ObjectCounter(BaseSolution):\n",
    "    \"\"\"\n",
    "        Clase para contar los objetos en un video.\n",
    "\n",
    "        Extend: BaseSolution\n",
    "        Atributos:\n",
    "         - in_count (int): Contador de objetos que entran.\n",
    "         - out_count (int): Contador de objetos que salen.\n",
    "         - counted_ids (List[int]): Lista de IDs de objetos contados.\n",
    "         - saved_ids (List[int]): Lista de IDs guardados en un archivo CSV.\n",
    "         - classwise_counts (Dict[str, Dict[str, int]]): Diccionario de conteos, categorizados por clase de objeto.\n",
    "         - region_initialized (bool): Indica si se ha inicializado la región de conteo en la imagen.\n",
    "         - show_in (bool): Controla la visualización del conteo de entrada.\n",
    "         - show_out (bool): Controla la visualización del conteo de salida.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Constructor de la clase.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_count = 0    \n",
    "        self.out_count = 0    \n",
    "        self.counted_ids = []    \n",
    "        self.saved_ids = []    \n",
    "        self.classwise_counts = {}     \n",
    "        self.region_initialized = False    \n",
    "\n",
    "        self.show_in = self.CFG.get(\"show_in\", True)\n",
    "        self.show_out = self.CFG.get(\"show_out\", True)\n",
    "\n",
    "    def save_label_to_csv(self, track_id, label, action):\n",
    "        \"\"\" Guarda en un archivo CSV los datos ['track_id', 'label', 'action', 'date', 'time'] \"\"\"\n",
    "        if track_id in self.saved_ids:\n",
    "            return  # Si el ID ya está registrado, se omite\n",
    "\n",
    "        # Obtener la fecha y hora actual\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")  \n",
    "\n",
    "        # Crear el archivo CSV con la fecha en el nombre\n",
    "        filename = f'./recursos/5_5/tracked_objects_{current_date}.csv'\n",
    "\n",
    "        # Comprobar si el archivo ya existe\n",
    "        file_exists = os.path.isfile(filename)\n",
    "\n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Escribir la cabecera si el archivo es nuevo\n",
    "            if not file_exists:\n",
    "                writer.writerow(['track_id', 'label', 'action', 'date', 'time'])\n",
    "\n",
    "            # Escribir una nueva línea con los datos\n",
    "            writer.writerow([track_id, label, action, current_time.split()[0], current_time.split()[1]])\n",
    "            self.saved_ids.append(track_id)  # Guardar el ID en la lista de guardados\n",
    "\n",
    "    def count_objects(self, current_centroid, track_id, prev_position, cls):\n",
    "        \"\"\"\n",
    "        Cuenta objetos en función de un área o línea de referencia.\n",
    "\n",
    "        Parámetros:\n",
    "         - current_centroid (Tuple[float, float]): Coordenadas del centroide en el fotograma actual.\n",
    "         - track_id (int): Identificador único del objeto rastreado.\n",
    "         - prev_position (Tuple[float, float]): Coordenadas de la posición en el fotograma anterior (x, y).\n",
    "         - cls (int): Índice de clase del objeto, utilizado para actualizar el conteo por clase.\n",
    "        \"\"\"\n",
    "        if prev_position is None or track_id in self.counted_ids:\n",
    "            return\n",
    "\n",
    "        action = None  \n",
    "\n",
    "        if len(self.region) == 2:  # Región lineal (definida como un segmento de línea)\n",
    "            line = self.LineString(self.region)\n",
    "            if line.intersects(self.LineString([prev_position, current_centroid])):\n",
    "                if abs(self.region[0][0] - self.region[1][0]) < abs(self.region[0][1] - self.region[1][1]):\n",
    "                    if current_centroid[0] > prev_position[0]:  # Movimiento hacia la derecha\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia la izquierda\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                else:\n",
    "                    if current_centroid[1] > prev_position[1]:  # Movimiento hacia abajo\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia arriba\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                self.counted_ids.append(track_id)\n",
    "\n",
    "        elif len(self.region) > 2:  # Región poligonal\n",
    "            polygon = self.Polygon(self.region)\n",
    "            if polygon.contains(self.Point(current_centroid)):\n",
    "                region_width = max([p[0] for p in self.region]) - min([p[0] for p in self.region])\n",
    "                region_height = max([p[1] for p in self.region]) - min([p[1] for p in self.region])\n",
    "\n",
    "                if region_width < region_height:\n",
    "                    if current_centroid[0] > prev_position[0]:  # Movimiento hacia la derecha\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia la izquierda\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                else:\n",
    "                    if current_centroid[1] > prev_position[1]:  # Movimiento hacia abajo\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia arriba\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                self.counted_ids.append(track_id)\n",
    "\n",
    "        # Guardar la etiqueta con la acción realizada\n",
    "        if action:\n",
    "            label = f\"{self.names[cls]} ID: {track_id}\"\n",
    "            self.save_label_to_csv(track_id, label, action)\n",
    "\n",
    "    def store_classwise_counts(self, cls):\n",
    "        \"\"\"Inicializa los conteos por clase si aún no existen.\"\"\"\n",
    "        if self.names[cls] not in self.classwise_counts:\n",
    "            self.classwise_counts[self.names[cls]] = {\"IN\": 0, \"OUT\": 0}\n",
    "\n",
    "    def display_counts(self, im0):\n",
    "        \"\"\"Muestra los conteos de objetos en la imagen de entrada.\"\"\"\n",
    "        labels_dict = {\n",
    "            str.capitalize(key): f\"{'IN ' + str(value['IN']) if self.show_in else ''} \"\n",
    "            f\"{'OUT ' + str(value['OUT']) if self.show_out else ''}\".strip()\n",
    "            for key, value in self.classwise_counts.items()\n",
    "            if value[\"IN\"] != 0 or value[\"OUT\"] != 0\n",
    "        }\n",
    "\n",
    "        if labels_dict:\n",
    "            self.annotator.display_analytics(im0, labels_dict, (104, 31, 17), (255, 255, 255), 10)\n",
    "\n",
    "        for track_id in self.track_ids:\n",
    "            if track_id in self.counted_ids:\n",
    "                in_count = self.in_count\n",
    "                label = f\"ID:{track_id} count at number {in_count}\"\n",
    "                self.annotator.box_label(self.boxes[self.track_ids.index(track_id)], label=label, color=(255, 255, 0))\n",
    "\n",
    "    def count(self, im0):\n",
    "        \"\"\"Procesa los datos de entrada (fotogramas o rastros de objetos) y actualiza los conteos.\"\"\"\n",
    "        if not self.region_initialized:\n",
    "            self.initialize_region()\n",
    "            self.region_initialized = True\n",
    "\n",
    "        self.annotator = Annotator(im0, line_width=self.line_width)\n",
    "        self.extract_tracks(im0)\n",
    "        self.annotator.draw_region(reg_pts=self.region, color=(104, 0, 123), thickness=self.line_width * 2)\n",
    "\n",
    "        for box, track_id, cls in zip(self.boxes, self.track_ids, self.clss):\n",
    "            self.store_tracking_history(track_id, box)\n",
    "            self.store_classwise_counts(cls)\n",
    "\n",
    "            label = f\"{self.names[cls]} ID: {track_id}\"\n",
    "            self.annotator.box_label(box, label=label, color=colors(cls, True))\n",
    "\n",
    "            current_centroid = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "            prev_position = self.track_history[track_id][-2] if len(self.track_history[track_id]) > 1 else None\n",
    "            self.count_objects(current_centroid, track_id, prev_position, cls)\n",
    "\n",
    "        self.display_counts(im0)\n",
    "        self.display_output(im0)\n",
    "\n",
    "        return im0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de callback del mouse\n",
    "def RGB(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE:  # Verificar el movimiento del mouse\n",
    "        point = [x, y]\n",
    "        print(f\"El mouse se movió a: {point}\") \n",
    "\n",
    "# Abrir el archivo de video\n",
    "cap = cv2.VideoCapture('/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/recursos/5_5/1.mp4')\n",
    "\n",
    "# Definir los puntos de la región para el conteo\n",
    "region_points = [(386,103), (458, 499)]\n",
    "\n",
    "# Inicializar el contador de objetos\n",
    "counter = ObjectCounter(\n",
    "    region = region_points,  # Pasar los puntos de la región\n",
    "    model = \"yolo11s.pt\",  \n",
    "    classes = [0],  # clase \"persona\"\n",
    "    show_in = True,  # conteo de entradas\n",
    "    show_out = True,  # conteo de salidas\n",
    "    line_width = 2,  #  grosor de la línea \n",
    ")\n",
    "\n",
    "# Crear ventana y establecer función callback del mouse\n",
    "cv2.namedWindow('RGB')\n",
    "cv2.setMouseCallback('RGB', RGB)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    # Leer un fotograma del video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        # Si el video termina, reiniciar desde el principio\n",
    "#        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "#        continue\n",
    "    count += 1\n",
    "    if count % 2 != 0:  # Omitir fotogramas impares\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # Procesar el fotograma con el contador de objetos\n",
    "    frame = counter.count(frame)\n",
    "\n",
    "    # Mostrar el fotograma\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Presionar 'q' para salir\n",
    "        break\n",
    "\n",
    "# Liberar el objeto de captura de video y cerrar la ventana de visualización\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 8\n",
    "Adapta el código anterior para que cuente vehículos en el video vehicle-counting.mp4 (tiene que contar por separado coches y camiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.solutions.solutions import BaseSolution\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class ObjectCounter(BaseSolution):\n",
    "    \"\"\"\n",
    "        Clase para contar los objetos en un video.\n",
    "\n",
    "        Extend: BaseSolution\n",
    "        Atributos:\n",
    "         - in_count (int): Contador de objetos que entran.\n",
    "         - out_count (int): Contador de objetos que salen.\n",
    "         - counted_ids (List[int]): Lista de IDs de objetos contados.\n",
    "         - saved_ids (List[int]): Lista de IDs guardados en un archivo CSV.\n",
    "         - classwise_counts (Dict[str, Dict[str, int]]): Diccionario de conteos, categorizados por clase de objeto.\n",
    "         - region_initialized (bool): Indica si se ha inicializado la región de conteo en la imagen.\n",
    "         - show_in (bool): Controla la visualización del conteo de entrada.\n",
    "         - show_out (bool): Controla la visualización del conteo de salida.\n",
    "         - csv_ruta (str): ruta del csv         ### CAMBIO\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ruta, **kwargs):         ### CAMBIO\n",
    "        \"\"\"Constructor de la clase.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_count = 0    \n",
    "        self.out_count = 0    \n",
    "        self.counted_ids = []    \n",
    "        self.saved_ids = []    \n",
    "        self.classwise_counts = {}     \n",
    "        self.region_initialized = False    \n",
    "        self.csv_ruta = ruta         ### CAMBIO\n",
    "\n",
    "        self.show_in = self.CFG.get(\"show_in\", True)\n",
    "        self.show_out = self.CFG.get(\"show_out\", True)\n",
    "\n",
    "    def save_label_to_csv(self, track_id, label, action):\n",
    "        \"\"\" Guarda en un archivo CSV los datos ['track_id', 'label', 'action', 'date', 'time'] \"\"\"\n",
    "        if track_id in self.saved_ids:\n",
    "            return  # Si el ID ya está registrado, se omite\n",
    "\n",
    "        # Obtener la fecha y hora actual\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")  \n",
    "\n",
    "        # Crear el archivo CSV con la fecha en el nombre\n",
    "        filename = self.csv_ruta+current_date+'.csv'    ### CAMBIO\n",
    "\n",
    "        # Comprobar si el archivo ya existe\n",
    "        file_exists = os.path.isfile(filename)\n",
    "\n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Escribir la cabecera si el archivo es nuevo\n",
    "            if not file_exists:\n",
    "                writer.writerow(['track_id', 'label', 'action', 'date', 'time'])\n",
    "\n",
    "            # Escribir una nueva línea con los datos\n",
    "            writer.writerow([track_id, label, action, current_time.split()[0], current_time.split()[1]])\n",
    "            self.saved_ids.append(track_id)  # Guardar el ID en la lista de guardados\n",
    "\n",
    "    def count_objects(self, current_centroid, track_id, prev_position, cls):\n",
    "        \"\"\"\n",
    "        Cuenta objetos en función de un área o línea de referencia.\n",
    "\n",
    "        Parámetros:\n",
    "         - current_centroid (Tuple[float, float]): Coordenadas del centroide en el fotograma actual.\n",
    "         - track_id (int): Identificador único del objeto rastreado.\n",
    "         - prev_position (Tuple[float, float]): Coordenadas de la posición en el fotograma anterior (x, y).\n",
    "         - cls (int): Índice de clase del objeto, utilizado para actualizar el conteo por clase.\n",
    "        \"\"\"\n",
    "        if prev_position is None or track_id in self.counted_ids:\n",
    "            return\n",
    "\n",
    "        action = None  \n",
    "\n",
    "        if len(self.region) == 2:  # Región lineal (definida como un segmento de línea)\n",
    "            line = self.LineString(self.region)\n",
    "            if line.intersects(self.LineString([prev_position, current_centroid])):\n",
    "                if abs(self.region[0][0] - self.region[1][0]) < abs(self.region[0][1] - self.region[1][1]):\n",
    "                    if current_centroid[0] > prev_position[0]:  # Movimiento hacia la derecha\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia la izquierda\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                else:\n",
    "                    if current_centroid[1] > prev_position[1]:  # Movimiento hacia abajo\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia arriba\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                self.counted_ids.append(track_id)\n",
    "\n",
    "        elif len(self.region) > 2:  # Región poligonal\n",
    "            polygon = self.Polygon(self.region)\n",
    "            if polygon.contains(self.Point(current_centroid)):\n",
    "                region_width = max([p[0] for p in self.region]) - min([p[0] for p in self.region])\n",
    "                region_height = max([p[1] for p in self.region]) - min([p[1] for p in self.region])\n",
    "\n",
    "                if region_width < region_height:\n",
    "                    if current_centroid[0] > prev_position[0]:  # Movimiento hacia la derecha\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia la izquierda\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                else:\n",
    "                    if current_centroid[1] > prev_position[1]:  # Movimiento hacia abajo\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento hacia arriba\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                self.counted_ids.append(track_id)\n",
    "\n",
    "        # Guardar la etiqueta con la acción realizada\n",
    "        if action:\n",
    "            label = f\"{self.names[cls]} ID: {track_id}\"\n",
    "            self.save_label_to_csv(track_id, label, action)\n",
    "\n",
    "    def store_classwise_counts(self, cls):\n",
    "        \"\"\"Inicializa los conteos por clase si aún no existen.\"\"\"\n",
    "        if self.names[cls] not in self.classwise_counts:\n",
    "            self.classwise_counts[self.names[cls]] = {\"IN\": 0, \"OUT\": 0}\n",
    "\n",
    "    def display_counts(self, im0):\n",
    "        \"\"\"Muestra los conteos de objetos en la imagen de entrada.\"\"\"\n",
    "        labels_dict = {\n",
    "            str.capitalize(key): f\"{'IN ' + str(value['IN']) if self.show_in else ''} \"\n",
    "            f\"{'OUT ' + str(value['OUT']) if self.show_out else ''}\".strip()\n",
    "            for key, value in self.classwise_counts.items()\n",
    "            if value[\"IN\"] != 0 or value[\"OUT\"] != 0\n",
    "        }\n",
    "\n",
    "        if labels_dict:\n",
    "            self.annotator.display_analytics(im0, labels_dict, (104, 31, 17), (255, 255, 255), 10)\n",
    "\n",
    "        for track_id in self.track_ids:\n",
    "            if track_id in self.counted_ids:\n",
    "                in_count = self.in_count\n",
    "                label = f\"ID:{track_id} count at number {in_count}\"\n",
    "                self.annotator.box_label(self.boxes[self.track_ids.index(track_id)], label=label, color=(255, 255, 0))\n",
    "\n",
    "    def count(self, im0):\n",
    "        \"\"\"Procesa los datos de entrada (fotogramas o rastros de objetos) y actualiza los conteos.\"\"\"\n",
    "        if not self.region_initialized:\n",
    "            self.initialize_region()\n",
    "            self.region_initialized = True\n",
    "\n",
    "        self.annotator = Annotator(im0, line_width=self.line_width)\n",
    "        self.extract_tracks(im0)\n",
    "        self.annotator.draw_region(reg_pts=self.region, color=(104, 0, 123), thickness=self.line_width * 2)\n",
    "\n",
    "        for box, track_id, cls in zip(self.boxes, self.track_ids, self.clss):\n",
    "            self.store_tracking_history(track_id, box)\n",
    "            self.store_classwise_counts(cls)\n",
    "\n",
    "            label = f\"{self.names[cls]} ID: {track_id}\"\n",
    "            self.annotator.box_label(box, label=label, color=colors(cls, True))\n",
    "\n",
    "            current_centroid = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "            prev_position = self.track_history[track_id][-2] if len(self.track_history[track_id]) > 1 else None\n",
    "            self.count_objects(current_centroid, track_id, prev_position, cls)\n",
    "\n",
    "        self.display_counts(im0)\n",
    "        self.display_output(im0)\n",
    "\n",
    "        return im0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de callback del mouse\n",
    "def RGB(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE:  # Verificar el movimiento del mouse\n",
    "        point = [x, y]\n",
    "        print(f\"El mouse se movió a: {point}\") \n",
    "\n",
    "# Abrir el archivo de video\n",
    "cap = cv2.VideoCapture('./recursos/5_5/vehicle-counting.mp4')   ### CAMBIO\n",
    "\n",
    "# Definir los puntos de la región para el conteo\n",
    "region_points = [(20,280), (1000, 280)]\n",
    "\n",
    "# Inicializar el contador de objetos\n",
    "counter = ObjectCounter(\n",
    "    ruta = './recursos/5_5/tracked_objects_autopista_',\n",
    "    region=region_points,  # Pasar los puntos de la región\n",
    "    model=\"./recursos/5_5/yolo/yolo11s.pt\",  \n",
    "    classes=[2,7],  # Detectar clases 2 car, 7 truk       ### CAMBIO\n",
    "    show_in=True,  # conteo de entradas\n",
    "    show_out=True,  # conteo de salidas\n",
    "    line_width=2,  # grosor de la línea \n",
    ")\n",
    "\n",
    "# Crear ventana y establecer función callback del mouse\n",
    "cv2.namedWindow('RGB')\n",
    "cv2.setMouseCallback('RGB', RGB)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    # Leer un fotograma del video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        # Si el video termina, reiniciar desde el principio\n",
    "#        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "#        continue\n",
    "    count += 1\n",
    "    if count % 2 != 0:  # Omitir fotogramas impares\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # Procesar el fotograma con el contador de objetos\n",
    "    frame = counter.count(frame)\n",
    "\n",
    "    # Mostrar el fotograma\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Presionar 'q' para salir\n",
    "        break\n",
    "\n",
    "# Liberar el objeto de captura de video y cerrar la ventana de visualización\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 9\n",
    "Adapta el ejercicio anterior para que en vez de coches cuente matrículas usando el modelo que has creado en el ejercicio 6. \n",
    "\n",
    "Si el video del apartado anterior no te sirve porque no se ven bien las matrículas crea un video con la IA generativa (si el video creado es demasiado corto junta varios videos en uno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  solo es necesario editar el segundo fichero\n",
    "\n",
    "# Definir la función de callback del mouse\n",
    "def RGB(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE:  # Verificar el movimiento del mouse\n",
    "        point = [x, y]\n",
    "        print(f\"El mouse se movió a: {point}\") \n",
    "\n",
    "# Abrir el archivo de video\n",
    "cap = cv2.VideoCapture('./recursos/5_5/prueba.mp4')   ### CAMBIO\n",
    "\n",
    "# Definir los puntos de la región para el conteo\n",
    "region_points = [(400,20), (800, 460)]\n",
    "\n",
    "# Inicializar el contador de objetos\n",
    "counter = ObjectCounter(\n",
    "    ruta = './recursos/5_5/tracked_objects_matriculas_',        ### CAMBIO\n",
    "    region=region_points,  # Pasar los puntos de la región\n",
    "    model='./recursos/5_5/yolo/yolo_matriculas.pt',  \n",
    "    classes=[0],  # Detectar clases \n",
    "    show_in=True,  # conteo de entradas\n",
    "    show_out=True,  # conteo de salidas\n",
    "    line_width=2,  # grosor de la línea \n",
    ")\n",
    "\n",
    "# Crear ventana y establecer función callback del mouse\n",
    "cv2.namedWindow('RGB')\n",
    "cv2.setMouseCallback('RGB', RGB)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    # Leer un fotograma del video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        # Si el video termina, reiniciar desde el principio\n",
    "#        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "#        continue\n",
    "    count += 1\n",
    "    if count % 2 != 0:  # Omitir fotogramas impares\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # Procesar el fotograma con el contador de objetos\n",
    "    frame = counter.count(frame)\n",
    "\n",
    "    # Mostrar el fotograma\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Presionar 'q' para salir\n",
    "        break\n",
    "\n",
    "# Liberar el objeto de captura de video y cerrar la ventana de visualización\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 10\n",
    "Vamos a hacer fine tuning sobre el modelo para segmentación SAM-2.\n",
    "\n",
    "Puedes sacar información de la siguiente página:\n",
    "\n",
    "https://blog.roboflow.com/fine-tune-sam-2-1/\n",
    "\n",
    "A la hora de conseguir el dataset, descargalo para no tener problemas con la API-KEY de Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   901  100   901    0     0   3090      0 --:--:-- --:--:-- --:--:--  3096\n",
      "100 4978k  100 4978k    0     0  2456k      0  0:00:02  0:00:02 --:--:-- 5458k\n",
      "Archive:  roboflow.zip\n",
      "replace README.dataset.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# obtener dataset\n",
    "!curl -L \"https://universe.roboflow.com/ds/p1fYx5Kd9a?key=NAS33NVLPw\" > roboflow.zip; unzip roboflow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO ES NECESARIO, ya tengo el dataset\n",
    "\n",
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "\n",
    "# rf = Roboflow(api_key=\"W8Wh3vwPre13GJ9ArQue\")\n",
    "# project = rf.workspace(\"brad-dwyer\").project(\"car-parts-pgo19\")\n",
    "# version = project.version(6)\n",
    "# dataset = version.download(\"sam2\")\n",
    "\n",
    "# # rename dataset.location to \"data\"\n",
    "# os.rename(dataset.location, \"/home/iabd/Escritorio/IABD/IA/SAPA/Redes Neuronales/Ej_sam2/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonando en 'sam2'...\n",
      "remote: Enumerating objects: 1070, done.\u001b[K\n",
      "remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n",
      "Recibiendo objetos: 100% (1070/1070), 134.70 MiB | 18.43 MiB/s, listo.\n",
      "Resolviendo deltas: 100% (375/375), listo.\n"
     ]
    }
   ],
   "source": [
    "# descargar sam2 (clonar repo) (ya esta hecho)\n",
    "!git clone https://github.com/facebookresearch/sam2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-10 20:09:22--  https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3\n",
      "Resolviendo drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.201.1, 2a00:1450:4003:80a::2001\n",
      "Conectando con drive.usercontent.google.com (drive.usercontent.google.com)[142.250.201.1]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 11055 (11K) [application/octet-stream]\n",
      "Guardando como: ‘./sam2/sam2/configs/train.yaml’\n",
      "\n",
      "./sam2/sam2/configs 100%[===================>]  10,80K  --.-KB/s    en 0,006s  \n",
      "\n",
      "2025-02-10 20:09:25 (1,65 MB/s) - ‘./sam2/sam2/configs/train.yaml’ guardado [11055/11055]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sam2 train.yaml\n",
    "!wget -O ./sam2/sam2/configs/train.yaml 'https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/EjSAM/sam2\n",
      "Obtaining file:///home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/EjSAM/sam2\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.5.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.20.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (4.67.1)\n",
      "Requirement already satisfied: hydra-core>=1.3.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (1.3.2)\n",
      "Requirement already satisfied: iopath>=0.1.10 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (0.1.10)\n",
      "Requirement already satisfied: pillow>=9.4.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (11.1.0)\n",
      "Requirement already satisfied: black==24.2.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (24.2.0)\n",
      "Requirement already satisfied: usort==1.0.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (1.0.2)\n",
      "Requirement already satisfied: ufmt==2.0.0b2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (2.0.0b2)\n",
      "Requirement already satisfied: fvcore>=0.1.5.post20221221 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (0.1.5.post20221221)\n",
      "Requirement already satisfied: pandas>=2.2.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (2.2.3)\n",
      "Requirement already satisfied: scikit-image>=0.24.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (0.25.1)\n",
      "Requirement already satisfied: tensorboard>=2.17.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (2.17.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.8 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (2.0.8)\n",
      "Requirement already satisfied: tensordict>=0.6.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (0.7.0)\n",
      "Requirement already satisfied: opencv-python>=4.7.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (4.10.0)\n",
      "Requirement already satisfied: submitit>=1.5.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from SAM-2==1.0) (1.5.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (8.1.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (24.2)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from black==24.2.0->SAM-2==1.0) (4.12.2)\n",
      "Requirement already satisfied: libcst>=0.4.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from ufmt==2.0.0b2->SAM-2==1.0) (1.6.0)\n",
      "Requirement already satisfied: moreorless>=0.4.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from ufmt==2.0.0b2->SAM-2==1.0) (0.4.0)\n",
      "Requirement already satisfied: tomlkit>=0.7.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from ufmt==2.0.0b2->SAM-2==1.0) (0.13.2)\n",
      "Requirement already satisfied: trailrunner>=1.2.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from ufmt==2.0.0b2->SAM-2==1.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=21.2.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from usort==1.0.2->SAM-2==1.0) (25.1.0)\n",
      "Requirement already satisfied: stdlibs>=2021.4.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from usort==1.0.2->SAM-2==1.0) (2024.12.3)\n",
      "Requirement already satisfied: toml>=0.10.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from usort==1.0.2->SAM-2==1.0) (0.10.2)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from fvcore>=0.1.5.post20221221->SAM-2==1.0) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from fvcore>=0.1.5.post20221221->SAM-2==1.0) (6.0.2)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from fvcore>=0.1.5.post20221221->SAM-2==1.0) (2.1.0)\n",
      "Requirement already satisfied: tabulate in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from fvcore>=0.1.5.post20221221->SAM-2==1.0) (0.9.0)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\n",
      "Requirement already satisfied: portalocker in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from iopath>=0.1.10->SAM-2==1.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from pandas>=2.2.2->SAM-2==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from pandas>=2.2.2->SAM-2==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from pandas>=2.2.2->SAM-2==1.0) (2023.3)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from pycocotools>=2.0.8->SAM-2==1.0) (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.11.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from scikit-image>=0.24.0->SAM-2==1.0) (1.15.1)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from scikit-image>=0.24.0->SAM-2==1.0) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from scikit-image>=0.24.0->SAM-2==1.0) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from scikit-image>=0.24.0->SAM-2==1.0) (2025.1.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from scikit-image>=0.24.0->SAM-2==1.0) (0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from submitit>=1.5.1->SAM-2==1.0) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (1.62.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (3.4.1)\n",
      "Requirement already satisfied: protobuf in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (72.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensorboard>=2.17.0->SAM-2==1.0) (3.1.3)\n",
      "Requirement already satisfied: orjson in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from tensordict>=0.6.0->SAM-2==1.0) (3.10.15)\n",
      "Requirement already satisfied: filelock in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (3.17.0)\n",
      "Requirement already satisfied: jinja2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.8->SAM-2==1.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.8->SAM-2==1.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.8->SAM-2==1.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.8->SAM-2==1.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.8->SAM-2==1.0) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.17.0->SAM-2==1.0) (3.0.2)\n",
      "Building wheels for collected packages: SAM-2\n",
      "  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for SAM-2: filename=SAM_2-1.0-0.editable-cp310-cp310-linux_x86_64.whl size=13838 sha256=49f70f9a09293917c35c62c53c021ce47c9da53cf16ade4d24797dae96f8438f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-edvkb1k8/wheels/75/35/fb/0593f7755f309cf0c5b966d2152161b6783757fc33eeff14ec\n",
      "Successfully built SAM-2\n",
      "Installing collected packages: SAM-2\n",
      "  Attempting uninstall: SAM-2\n",
      "    Found existing installation: SAM-2 1.0\n",
      "    Uninstalling SAM-2-1.0:\n",
      "      Successfully uninstalled SAM-2-1.0\n",
      "Successfully installed SAM-2-1.0\n"
     ]
    }
   ],
   "source": [
    "# navigate to the sam2 directory and install the model as well as supervision\n",
    "%cd ./sam2/\n",
    "\n",
    "# ya esta instalado\n",
    "!pip install -e .[dev]\n",
    "!pip install supervision -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sam2.1_hiera_tiny.pt checkpoint...\n",
      "--2025-02-10 20:10:24--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\n",
      "Resolviendo dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.41.12, 18.154.41.57, 18.154.41.96, ...\n",
      "Conectando con dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)[18.154.41.12]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 156008466 (149M) [application/vnd.snesdev-page-table]\n",
      "Guardando como: ‘sam2.1_hiera_tiny.pt’\n",
      "\n",
      "sam2.1_hiera_tiny.p 100%[===================>] 148,78M  47,5MB/s    en 3,3s    \n",
      "\n",
      "2025-02-10 20:10:27 (45,7 MB/s) - ‘sam2.1_hiera_tiny.pt’ guardado [156008466/156008466]\n",
      "\n",
      "Downloading sam2.1_hiera_small.pt checkpoint...\n",
      "--2025-02-10 20:10:27--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n",
      "Resolviendo dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.41.8, 18.154.41.57, 18.154.41.96, ...\n",
      "Conectando con dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)[18.154.41.8]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 184416285 (176M) [application/vnd.snesdev-page-table]\n",
      "Guardando como: ‘sam2.1_hiera_small.pt’\n",
      "\n",
      "sam2.1_hiera_small. 100%[===================>] 175,87M  53,1MB/s    en 3,4s    \n",
      "\n",
      "2025-02-10 20:10:31 (51,7 MB/s) - ‘sam2.1_hiera_small.pt’ guardado [184416285/184416285]\n",
      "\n",
      "Downloading sam2.1_hiera_base_plus.pt checkpoint...\n",
      "--2025-02-10 20:10:31--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt\n",
      "Resolviendo dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.41.96, 18.154.41.8, 18.154.41.57, ...\n",
      "Conectando con dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)[18.154.41.96]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 323606802 (309M) [application/vnd.snesdev-page-table]\n",
      "Guardando como: ‘sam2.1_hiera_base_plus.pt’\n",
      "\n",
      "sam2.1_hiera_base_p 100%[===================>] 308,62M  59,1MB/s    en 5,6s    \n",
      "\n",
      "2025-02-10 20:10:36 (55,0 MB/s) - ‘sam2.1_hiera_base_plus.pt’ guardado [323606802/323606802]\n",
      "\n",
      "Downloading sam2.1_hiera_large.pt checkpoint...\n",
      "--2025-02-10 20:10:36--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
      "Resolviendo dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.41.96, 18.154.41.57, 18.154.41.12, ...\n",
      "Conectando con dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)[18.154.41.96]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 898083611 (856M) [application/vnd.snesdev-page-table]\n",
      "Guardando como: ‘sam2.1_hiera_large.pt’\n",
      "\n",
      "sam2.1_hiera_large. 100%[===================>] 856,48M  58,9MB/s    en 15s     \n",
      "\n",
      "2025-02-10 20:10:52 (55,5 MB/s) - ‘sam2.1_hiera_large.pt’ guardado [898083611/898083611]\n",
      "\n",
      "All checkpoints are downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# download the checkpoints\n",
    "!cd ./checkpoints && ./download_ckpts.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t\t    docker-compose.yaml  pyproject.toml    setup.py\n",
      "backend.Dockerfile  INSTALL.md\t\t README.md\t   tools\n",
      "checkpoints\t    LICENSE\t\t RELEASE_NOTES.md  training\n",
      "CODE_OF_CONDUCT.md  LICENSE_cctorch\t sam2\n",
      "CONTRIBUTING.md     MANIFEST.in\t\t SAM_2.egg-info\n",
      "demo\t\t    notebooks\t\t sav_dataset\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# preparar dataset\n",
    "import re\n",
    "\n",
    "# FOLDER = \"../train\"\n",
    "FOLDER = \"./content/data/train\"\n",
    "for filename in os.listdir(FOLDER):\n",
    "    # Replace all except last dot with underscore\n",
    "    new_filename = filename.replace(\".\", \"_\", filename.count(\".\") - 1)\n",
    "    if not re.search(r\"_\\d+\\.\\w+$\", new_filename):\n",
    "        # Add an int to the end of base name\n",
    "        new_filename = new_filename.replace(\".\", \"_1.\")\n",
    "    os.rename(os.path.join(FOLDER, filename), os.path.join(FOLDER, new_filename)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparar dataset\n",
    "import re\n",
    "\n",
    "rutas = [\"/home/iabd/Escritorio/clase/Car Parts.v6i.sam2/train\",\n",
    "    \"/home/iabd/Escritorio/clase/Car Parts.v6i.sam2/test\",\n",
    "    \"/home/iabd/Escritorio/clase/Car Parts.v6i.sam2/valid\"]\n",
    "\n",
    "for carpeta in rutas:\n",
    "    for filename in os.listdir(carpeta):\n",
    "        # Replace all except last dot with underscore\n",
    "        new_filename = filename.replace(\".\", \"_\", filename.count(\".\") - 1)\n",
    "        if not re.search(r\"_\\d+\\.\\w+$\", new_filename):\n",
    "            # Add an int to the end of base name\n",
    "            new_filename = new_filename.replace(\".\", \"_1.\")\n",
    "        os.rename(os.path.join(carpeta, filename), os.path.join(carpeta, new_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/Ej10/sam2\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t\t    demo\t\t MANIFEST.in\t   sam2\n",
      "backend.Dockerfile  docker-compose.yaml  notebooks\t   sav_dataset\n",
      "checkpoints\t    INSTALL.md\t\t pyproject.toml    setup.py\n",
      "CODE_OF_CONDUCT.md  LICENSE\t\t README.md\t   tools\n",
      "CONTRIBUTING.md     LICENSE_cctorch\t RELEASE_NOTES.md  training\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optree>=0.13.0\n",
      "  Downloading optree-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from optree>=0.13.0) (4.12.2)\n",
      "Downloading optree-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "Installing collected packages: optree\n",
      "  Attempting uninstall: optree\n",
      "    Found existing installation: optree 0.12.1\n",
      "    Uninstalling optree-0.12.1:\n",
      "      Successfully uninstalled optree-0.12.1\n",
      "Successfully installed optree-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m pip install --upgrade 'optree>=0.13.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### Train App Config ####################\n",
      "scratch:\n",
      "  resolution: 1024\n",
      "  train_batch_size: 1\n",
      "  num_train_workers: 10\n",
      "  num_frames: 1\n",
      "  max_num_objects: 3\n",
      "  base_lr: 5.0e-06\n",
      "  vision_lr: 3.0e-06\n",
      "  phases_per_epoch: 1\n",
      "  num_epochs: 40\n",
      "dataset:\n",
      "  img_folder: /home/iabd/Escritorio/clase/Car Parts.v6i.sam2/train\n",
      "  gt_folder: /home/iabd/Escritorio/clase/Car Parts.v6i.sam2/train\n",
      "  multiplier: 2\n",
      "vos:\n",
      "  train_transforms:\n",
      "  - _target_: training.dataset.transforms.ComposeAPI\n",
      "    transforms:\n",
      "    - _target_: training.dataset.transforms.RandomHorizontalFlip\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.RandomAffine\n",
      "      degrees: 25\n",
      "      shear: 20\n",
      "      image_interpolation: bilinear\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.RandomResizeAPI\n",
      "      sizes: ${scratch.resolution}\n",
      "      square: true\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.ColorJitter\n",
      "      consistent_transform: true\n",
      "      brightness: 0.1\n",
      "      contrast: 0.03\n",
      "      saturation: 0.03\n",
      "      hue: null\n",
      "    - _target_: training.dataset.transforms.RandomGrayscale\n",
      "      p: 0.05\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.ColorJitter\n",
      "      consistent_transform: false\n",
      "      brightness: 0.1\n",
      "      contrast: 0.05\n",
      "      saturation: 0.05\n",
      "      hue: null\n",
      "    - _target_: training.dataset.transforms.ToTensorAPI\n",
      "    - _target_: training.dataset.transforms.NormalizeAPI\n",
      "      mean:\n",
      "      - 0.485\n",
      "      - 0.456\n",
      "      - 0.406\n",
      "      std:\n",
      "      - 0.229\n",
      "      - 0.224\n",
      "      - 0.225\n",
      "trainer:\n",
      "  _target_: training.trainer.Trainer\n",
      "  mode: train_only\n",
      "  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n",
      "  accelerator: cuda\n",
      "  seed_value: 123\n",
      "  model:\n",
      "    _target_: training.model.sam2.SAM2Train\n",
      "    image_encoder:\n",
      "      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n",
      "      scalp: 1\n",
      "      trunk:\n",
      "        _target_: sam2.modeling.backbones.hieradet.Hiera\n",
      "        embed_dim: 112\n",
      "        num_heads: 2\n",
      "        drop_path_rate: 0.1\n",
      "      neck:\n",
      "        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n",
      "        position_encoding:\n",
      "          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
      "          num_pos_feats: 256\n",
      "          normalize: true\n",
      "          scale: null\n",
      "          temperature: 10000\n",
      "        d_model: 256\n",
      "        backbone_channel_list:\n",
      "        - 896\n",
      "        - 448\n",
      "        - 224\n",
      "        - 112\n",
      "        fpn_top_down_levels:\n",
      "        - 2\n",
      "        - 3\n",
      "        fpn_interp_model: nearest\n",
      "    memory_attention:\n",
      "      _target_: sam2.modeling.memory_attention.MemoryAttention\n",
      "      d_model: 256\n",
      "      pos_enc_at_input: true\n",
      "      layer:\n",
      "        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n",
      "        activation: relu\n",
      "        dim_feedforward: 2048\n",
      "        dropout: 0.1\n",
      "        pos_enc_at_attn: false\n",
      "        self_attention:\n",
      "          _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
      "          rope_theta: 10000.0\n",
      "          feat_sizes:\n",
      "          - 32\n",
      "          - 32\n",
      "          embedding_dim: 256\n",
      "          num_heads: 1\n",
      "          downsample_rate: 1\n",
      "          dropout: 0.1\n",
      "        d_model: 256\n",
      "        pos_enc_at_cross_attn_keys: true\n",
      "        pos_enc_at_cross_attn_queries: false\n",
      "        cross_attention:\n",
      "          _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
      "          rope_theta: 10000.0\n",
      "          feat_sizes:\n",
      "          - 32\n",
      "          - 32\n",
      "          rope_k_repeat: true\n",
      "          embedding_dim: 256\n",
      "          num_heads: 1\n",
      "          downsample_rate: 1\n",
      "          dropout: 0.1\n",
      "          kv_in_dim: 64\n",
      "      num_layers: 4\n",
      "    memory_encoder:\n",
      "      _target_: sam2.modeling.memory_encoder.MemoryEncoder\n",
      "      out_dim: 64\n",
      "      position_encoding:\n",
      "        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
      "        num_pos_feats: 64\n",
      "        normalize: true\n",
      "        scale: null\n",
      "        temperature: 10000\n",
      "      mask_downsampler:\n",
      "        _target_: sam2.modeling.memory_encoder.MaskDownSampler\n",
      "        kernel_size: 3\n",
      "        stride: 2\n",
      "        padding: 1\n",
      "      fuser:\n",
      "        _target_: sam2.modeling.memory_encoder.Fuser\n",
      "        layer:\n",
      "          _target_: sam2.modeling.memory_encoder.CXBlock\n",
      "          dim: 256\n",
      "          kernel_size: 7\n",
      "          padding: 3\n",
      "          layer_scale_init_value: 1.0e-06\n",
      "          use_dwconv: true\n",
      "        num_layers: 2\n",
      "    num_maskmem: 7\n",
      "    image_size: ${scratch.resolution}\n",
      "    sigmoid_scale_for_mem_enc: 20.0\n",
      "    sigmoid_bias_for_mem_enc: -10.0\n",
      "    use_mask_input_as_output_without_sam: true\n",
      "    directly_add_no_mem_embed: true\n",
      "    no_obj_embed_spatial: true\n",
      "    use_high_res_features_in_sam: true\n",
      "    multimask_output_in_sam: true\n",
      "    iou_prediction_use_sigmoid: true\n",
      "    use_obj_ptrs_in_encoder: true\n",
      "    add_tpos_enc_to_obj_ptrs: true\n",
      "    proj_tpos_enc_in_obj_ptrs: true\n",
      "    use_signed_tpos_enc_to_obj_ptrs: true\n",
      "    only_obj_ptrs_in_the_past_for_eval: true\n",
      "    pred_obj_scores: true\n",
      "    pred_obj_scores_mlp: true\n",
      "    fixed_no_obj_ptr: true\n",
      "    multimask_output_for_tracking: true\n",
      "    use_multimask_token_for_obj_ptr: true\n",
      "    multimask_min_pt_num: 0\n",
      "    multimask_max_pt_num: 1\n",
      "    use_mlp_for_obj_ptr_proj: true\n",
      "    prob_to_use_pt_input_for_train: 0.5\n",
      "    prob_to_use_pt_input_for_eval: 0.0\n",
      "    prob_to_use_box_input_for_train: 0.5\n",
      "    prob_to_use_box_input_for_eval: 0.0\n",
      "    prob_to_sample_from_gt_for_train: 0.1\n",
      "    num_frames_to_correct_for_train: 2\n",
      "    num_frames_to_correct_for_eval: 1\n",
      "    rand_frames_to_correct_for_train: true\n",
      "    add_all_frames_to_correct_as_cond: true\n",
      "    num_init_cond_frames_for_train: 2\n",
      "    rand_init_cond_frames_for_train: true\n",
      "    num_correction_pt_per_frame: 7\n",
      "    use_act_ckpt_iterative_pt_sampling: false\n",
      "    num_init_cond_frames_for_eval: 1\n",
      "    forward_backbone_per_frame_for_eval: true\n",
      "  data:\n",
      "    train:\n",
      "      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n",
      "      phases_per_epoch: ${scratch.phases_per_epoch}\n",
      "      batch_sizes:\n",
      "      - ${scratch.train_batch_size}\n",
      "      datasets:\n",
      "      - _target_: training.dataset.vos_dataset.VOSDataset\n",
      "        transforms: ${vos.train_transforms}\n",
      "        training: true\n",
      "        video_dataset:\n",
      "          _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n",
      "          img_folder: ${dataset.img_folder}\n",
      "          gt_folder: ${dataset.gt_folder}\n",
      "        multiplier: ${dataset.multiplier}\n",
      "        sampler:\n",
      "          _target_: training.dataset.vos_sampler.RandomUniformSampler\n",
      "          num_frames: 1\n",
      "          max_num_objects: ${scratch.max_num_objects}\n",
      "      shuffle: true\n",
      "      num_workers: ${scratch.num_train_workers}\n",
      "      pin_memory: true\n",
      "      drop_last: true\n",
      "      collate_fn:\n",
      "        _target_: training.utils.data_utils.collate_fn\n",
      "        _partial_: true\n",
      "        dict_key: all\n",
      "  optim:\n",
      "    amp:\n",
      "      enabled: true\n",
      "      amp_dtype: bfloat16\n",
      "    optimizer:\n",
      "      _target_: torch.optim.AdamW\n",
      "    gradient_clip:\n",
      "      _target_: training.optimizer.GradientClipper\n",
      "      max_norm: 0.1\n",
      "      norm_type: 2\n",
      "    param_group_modifiers:\n",
      "    - _target_: training.optimizer.layer_decay_param_modifier\n",
      "      _partial_: true\n",
      "      layer_decay_value: 0.9\n",
      "      apply_to: image_encoder.trunk\n",
      "      overrides:\n",
      "      - pattern: '*pos_embed*'\n",
      "        value: 1.0\n",
      "    options:\n",
      "      lr:\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n",
      "          start_value: ${scratch.base_lr}\n",
      "          end_value: ${divide:${scratch.base_lr},10}\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n",
      "          start_value: ${scratch.vision_lr}\n",
      "          end_value: ${divide:${scratch.vision_lr},10}\n",
      "        param_names:\n",
      "        - image_encoder.*\n",
      "      weight_decay:\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n",
      "          value: 0.1\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n",
      "          value: 0.0\n",
      "        param_names:\n",
      "        - '*bias*'\n",
      "        module_cls_names:\n",
      "        - torch.nn.LayerNorm\n",
      "  loss:\n",
      "    all:\n",
      "      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n",
      "      weight_dict:\n",
      "        loss_mask: 20\n",
      "        loss_dice: 1\n",
      "        loss_iou: 1\n",
      "        loss_class: 1\n",
      "      supervise_all_iou: true\n",
      "      iou_use_l1_loss: true\n",
      "      pred_obj_scores: true\n",
      "      focal_gamma_obj_score: 0.0\n",
      "      focal_alpha_obj_score: -1.0\n",
      "  distributed:\n",
      "    backend: nccl\n",
      "    find_unused_parameters: true\n",
      "  logging:\n",
      "    tensorboard_writer:\n",
      "      _target_: training.utils.logger.make_tensorboard_logger\n",
      "      log_dir: ${launcher.experiment_log_dir}/tensorboard\n",
      "      flush_secs: 120\n",
      "      should_log: true\n",
      "    log_dir: ${launcher.experiment_log_dir}/logs\n",
      "    log_freq: 10\n",
      "  checkpoint:\n",
      "    save_dir: ${launcher.experiment_log_dir}/checkpoints\n",
      "    save_freq: 0\n",
      "    model_weight_initializer:\n",
      "      _partial_: true\n",
      "      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n",
      "      strict: true\n",
      "      ignore_unexpected_keys: null\n",
      "      ignore_missing_keys: null\n",
      "      state_dict:\n",
      "        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n",
      "        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt\n",
      "        ckpt_state_dict_keys:\n",
      "        - model\n",
      "launcher:\n",
      "  num_nodes: 1\n",
      "  gpus_per_node: 8\n",
      "  experiment_log_dir: /home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/EjSAM/sam2/sam2_logs/configs/train.yaml\n",
      "submitit:\n",
      "  partition: null\n",
      "  account: null\n",
      "  qos: null\n",
      "  cpus_per_task: 10\n",
      "  use_cluster: false\n",
      "  timeout_hour: 24\n",
      "  name: null\n",
      "  port_range:\n",
      "  - 10000\n",
      "  - 65000\n",
      "\n",
      "############################################################\n",
      "INFO 2025-02-10 20:13:44,326 train_utils.py: 108: MACHINE SEED: 4920\n",
      "INFO 2025-02-10 20:13:44,327 train_utils.py: 154: Logging ENV_VARIABLES\n",
      "INFO 2025-02-10 20:13:44,328 train_utils.py: 155: CHROME_DESKTOP=code.desktop\n",
      "CLICOLOR=1\n",
      "CLICOLOR_FORCE=1\n",
      "CONDA_ALLOW_SOFTLINKS=false\n",
      "CONDA_DEFAULT_ENV=iabd_3_10_SAM2\n",
      "CONDA_EXE=/home/iabd/anaconda3/bin/conda\n",
      "CONDA_PREFIX=/home/iabd/anaconda3/envs/iabd_3_10_SAM2\n",
      "CONDA_PREFIX_1=/home/iabd/anaconda3\n",
      "CONDA_PROMPT_MODIFIER=(iabd_3_10_SAM2) \n",
      "CONDA_PYTHON_EXE=/home/iabd/anaconda3/bin/python\n",
      "CONDA_ROOT=/home/iabd/anaconda3\n",
      "CONDA_SHLVL=2\n",
      "CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
      "CUDA_MODULE_LOADING=LAZY\n",
      "DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1001/bus\n",
      "DEBUGINFOD_URLS=https://debuginfod.ubuntu.com \n",
      "DESKTOP_SESSION=ubuntu\n",
      "DISPLAY=:1\n",
      "ELECTRON_RUN_AS_NODE=1\n",
      "FORCE_COLOR=1\n",
      "GDK_BACKEND=x11\n",
      "GDMSESSION=ubuntu\n",
      "GIO_LAUNCHED_DESKTOP_FILE=/usr/share/applications/code.desktop\n",
      "GIO_LAUNCHED_DESKTOP_FILE_PID=28660\n",
      "GIT_PAGER=cat\n",
      "GJS_DEBUG_OUTPUT=stderr\n",
      "GJS_DEBUG_TOPICS=JS ERROR;JS LOG\n",
      "GNOME_DESKTOP_SESSION_ID=this-is-deprecated\n",
      "GNOME_SHELL_SESSION_MODE=ubuntu\n",
      "GPG_AGENT_INFO=/run/user/1001/gnupg/S.gpg-agent:0:1\n",
      "GSETTINGS_SCHEMA_DIR=/home/iabd/anaconda3/envs/iabd_3_10_SAM2/share/glib-2.0/schemas\n",
      "GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=\n",
      "GSM_SKIP_SSH_AGENT_WORKAROUND=true\n",
      "GTK_MODULES=gail:atk-bridge\n",
      "HOME=/home/iabd\n",
      "HYDRA_FULL_ERROR=1\n",
      "IM_CONFIG_PHASE=1\n",
      "INVOCATION_ID=9fe8eb2e052e4b26bdfde9edf447d332\n",
      "JOURNAL_STREAM=8:422\n",
      "KINETO_LOG_LEVEL=5\n",
      "LANG=es_ES.UTF-8\n",
      "LESSCLOSE=/usr/bin/lesspipe %s %s\n",
      "LESSOPEN=| /usr/bin/lesspipe %s\n",
      "LOCAL_RANK=0\n",
      "LOGNAME=iabd\n",
      "MANAGERPID=2606\n",
      "MASTER_ADDR=localhost\n",
      "MASTER_PORT=54599\n",
      "MEMKIND_HEAP_MANAGER=TBB\n",
      "MEMORY_PRESSURE_WATCH=/sys/fs/cgroup/user.slice/user-1001.slice/user@1001.service/session.slice/org.gnome.Shell@x11.service/memory.pressure\n",
      "MEMORY_PRESSURE_WRITE=c29tZSAyMDAwMDAgMjAwMDAwMAA=\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "NUMEXPR_MAX_THREADS=8\n",
      "OMP_NUM_THREADS=1\n",
      "ORIGINAL_XDG_CURRENT_DESKTOP=ubuntu:GNOME\n",
      "PAGER=cat\n",
      "PATH=/home/iabd/anaconda3/envs/iabd_3_10_SAM2/bin:/home/iabd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin\n",
      "PWD=/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/EjSAM/sam2\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "PYTHONIOENCODING=utf-8\n",
      "PYTHONUNBUFFERED=1\n",
      "PYTHON_FROZEN_MODULES=on\n",
      "QT_ACCESSIBILITY=1\n",
      "QT_IM_MODULE=ibus\n",
      "RANK=0\n",
      "SESSION_MANAGER=local/iadb-17:@/tmp/.ICE-unix/2967,unix/iadb-17:/tmp/.ICE-unix/2967\n",
      "SHELL=/bin/bash\n",
      "SHLVL=1\n",
      "SSH_AUTH_SOCK=/run/user/1001/keyring/ssh\n",
      "SYSTEMD_EXEC_PID=3038\n",
      "TERM=xterm-color\n",
      "TESSDATA_PREFIX=/home/iabd/anaconda3/envs/iabd_3_10_SAM2/share/tessdata/\n",
      "TF2_BEHAVIOR=1\n",
      "TF_CPP_MIN_LOG_LEVEL=3\n",
      "TORCH_CPP_LOG_LEVEL=ERROR\n",
      "TORCH_NCCL_ASYNC_ERROR_HANDLING=1\n",
      "TPU_ML_PLATFORM=Tensorflow\n",
      "TPU_ML_PLATFORM_VERSION=2.17.0\n",
      "USER=iabd\n",
      "USERNAME=iabd\n",
      "VSCODE_CODE_CACHE_PATH=/home/iabd/.config/Code/CachedData/91fbdddc47bc9c09064bf7acf133d22631cbf083\n",
      "VSCODE_CRASH_REPORTER_PROCESS_TYPE=extensionHost\n",
      "VSCODE_CWD=/home/iabd\n",
      "VSCODE_ESM_ENTRYPOINT=vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS=true\n",
      "VSCODE_IPC_HOOK=/run/user/1001/vscode-79fcfdaa-1.96-main.sock\n",
      "VSCODE_NLS_CONFIG={\"userLocale\":\"en-us\",\"osLocale\":\"es-es\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/usr/share/code/resources/app/out/nls.messages.json\",\"locale\":\"en-us\",\"availableLanguages\":{}}\n",
      "VSCODE_PID=28660\n",
      "WINDOWPATH=2\n",
      "WORLD_SIZE=1\n",
      "XAUTHORITY=/run/user/1001/gdm/Xauthority\n",
      "XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg\n",
      "XDG_CURRENT_DESKTOP=Unity\n",
      "XDG_DATA_DIRS=/usr/share/ubuntu:/usr/share/gnome:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop\n",
      "XDG_MENU_PREFIX=gnome-\n",
      "XDG_RUNTIME_DIR=/run/user/1001\n",
      "XDG_SESSION_CLASS=user\n",
      "XDG_SESSION_DESKTOP=ubuntu\n",
      "XDG_SESSION_TYPE=x11\n",
      "XMODIFIERS=@im=ibus\n",
      "_=/home/iabd/anaconda3/envs/iabd_3_10_SAM2/bin/python\n",
      "_CE_CONDA=\n",
      "_CE_M=\n",
      "\n",
      "INFO 2025-02-10 20:13:44,328 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-02-10 20:13:44,329 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/EjSAM/sam2/sam2_logs/configs/train.yaml/tensorboard\n",
      "INFO 2025-02-10 20:13:45,046 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5\n",
      "INFO 2025-02-10 20:13:45,049 trainer.py:1059: ====================\n",
      "INFO 2025-02-10 20:13:45,049 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>\n",
      "INFO 2025-02-10 20:13:45,051 trainer.py:1061: Model is SAM2Train(\n",
      "  (image_encoder): ImageEncoder(\n",
      "    (trunk): Hiera(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
      "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
      "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
      "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
      "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (2): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=112, out_features=672, bias=True)\n",
      "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
      "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=112, out_features=224, bias=True)\n",
      "        )\n",
      "        (3-4): 2 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=224, out_features=672, bias=True)\n",
      "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
      "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (5): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=224, out_features=1344, bias=True)\n",
      "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
      "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=224, out_features=448, bias=True)\n",
      "        )\n",
      "        (6-20): 15 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
      "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
      "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (21): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=448, out_features=2688, bias=True)\n",
      "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
      "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=448, out_features=896, bias=True)\n",
      "        )\n",
      "        (22-23): 2 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=896, out_features=2688, bias=True)\n",
      "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
      "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): FpnNeck(\n",
      "      (position_encoding): PositionEmbeddingSine()\n",
      "      (convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "  (memory_attention): MemoryAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x MemoryAttentionLayer(\n",
      "        (self_attn): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (cross_attn_image): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (memory_encoder): MemoryEncoder(\n",
      "    (mask_downsampler): MaskDownSampler(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): LayerNorm2d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): LayerNorm2d()\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (7): LayerNorm2d()\n",
      "        (8): GELU(approximate='none')\n",
      "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (10): LayerNorm2d()\n",
      "        (11): GELU(approximate='none')\n",
      "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fuser): Fuser(\n",
      "      (proj): Identity()\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x CXBlock(\n",
      "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm2d()\n",
      "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (position_encoding): PositionEmbeddingSine()\n",
      "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (sam_prompt_encoder): PromptEncoder(\n",
      "    (pe_layer): PositionEmbeddingRandom()\n",
      "    (point_embeddings): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "    (mask_downscaling): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): LayerNorm2d()\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (sam_mask_decoder): MaskDecoder(\n",
      "    (transformer): TwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TwoWayAttentionBlock(\n",
      "          (self_attn): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): Attention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (obj_score_token): Embedding(1, 256)\n",
      "    (output_upscaling): Sequential(\n",
      "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): GELU(approximate='none')\n",
      "    )\n",
      "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "        (act): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (pred_obj_score_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (obj_ptr_proj): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "INFO 2025-02-10 20:13:45,051 trainer.py:1062: \tTotal parameters 80.9 M\n",
      "INFO 2025-02-10 20:13:45,051 trainer.py:1063: \tTrainable parameters 80.9 M\n",
      "INFO 2025-02-10 20:13:45,051 trainer.py:1066: \tNon-Trainable parameters 0  \n",
      "INFO 2025-02-10 20:13:45,051 trainer.py:1069: ====================\n",
      "INFO 2025-02-10 20:13:45,054 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-02-10 20:13:45,054 trainer.py: 314: Moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-02-10 20:13:45,149 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-02-10 20:13:45,161 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.proj.weight', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.5.attn.qkv.weight'}\n",
      "INFO 2025-02-10 20:13:45,164 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'mask_downsample.bias', 'memory_attention.layers.3.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'memory_attention.layers.1.norm1.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'memory_encoder.pix_feat_proj.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'image_encoder.neck.convs.1.conv.bias', 'memory_attention.layers.0.linear1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_attention.layers.0.norm2.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'obj_ptr_proj.layers.1.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'obj_ptr_proj.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'sam_mask_decoder.conv_s1.bias', 'memory_attention.layers.0.linear2.bias', 'memory_attention.layers.3.linear2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.layers.1.linear2.bias', 'memory_attention.norm.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'memory_attention.layers.2.linear1.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.neck.convs.0.conv.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'memory_attention.layers.0.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'memory_attention.layers.3.linear1.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'memory_attention.layers.2.linear2.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.0.norm3.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'memory_encoder.out_proj.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'sam_mask_decoder.conv_s0.bias', 'memory_attention.layers.2.norm3.bias', 'obj_ptr_tpos_proj.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'memory_attention.layers.3.norm3.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias'}\n",
      "INFO 2025-02-10 20:13:45,165 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_attention.layers.3.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'memory_attention.layers.3.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'memory_attention.layers.1.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'memory_attention.layers.2.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'memory_attention.layers.0.norm1.weight', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'memory_attention.layers.3.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'memory_attention.layers.2.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.16.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'memory_attention.layers.2.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.1.norm3.bias', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_attention.layers.2.norm3.weight', 'memory_attention.layers.2.norm3.bias', 'memory_attention.layers.1.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.13.norm2.weight'} \n",
      "Raw dataset length = 38\n",
      "INFO 2025-02-10 20:13:45,448 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n",
      "INFO 2025-02-10 20:13:45,631 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': './checkpoints/sam2.1_hiera_base_plus.pt', 'ckpt_state_dict_keys': ['model']}}\n",
      "/home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/iabd/Escritorio/clase/iabd/ia/sapa/deep_learning/EjSAM/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(\n",
      "/home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages/torch/autograd/graph.py:823: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\n",
      "bucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "INFO 2025-02-10 20:14:22,620 train_utils.py: 271: Train Epoch: [0][ 0/38] | Batch Time: 36.73 (36.73) | Data Time: 32.43 (32.43) | Mem (GB): 6.00 (6.00/6.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 1.45e+00 (1.45e+00)\n",
      "INFO 2025-02-10 20:14:29,471 train_utils.py: 271: Train Epoch: [0][10/38] | Batch Time: 0.70 (3.96) | Data Time: 0.00 (2.95) | Mem (GB): 7.00 (6.91/7.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 5.85e+00 (2.32e+00)\n",
      "INFO 2025-02-10 20:14:35,374 train_utils.py: 271: Train Epoch: [0][20/38] | Batch Time: 0.41 (2.36) | Data Time: 0.00 (1.55) | Mem (GB): 7.00 (6.95/7.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 5.09e+00 (2.74e+00)\n",
      "INFO 2025-02-10 20:14:39,643 train_utils.py: 271: Train Epoch: [0][30/38] | Batch Time: 0.45 (1.73) | Data Time: 0.00 (1.05) | Mem (GB): 7.00 (6.97/7.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 9.63e-01 (2.71e+00)\n",
      "INFO 2025-02-10 20:14:53,484 trainer.py: 950: Estimated time remaining: 00d 00h 36m\n",
      "INFO 2025-02-10 20:14:53,485 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:14:53,485 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 2.7031852132395695, 'Losses/train_all_loss_mask': 0.052548695809060804, 'Losses/train_all_loss_dice': 1.0562067302434068, 'Losses/train_all_loss_iou': 0.5960045423554746, 'Losses/train_all_loss_class': 3.935799492200745e-08, 'Losses/train_all_core_loss': 2.7031852132395695, 'Trainer/where': 0.024342105263157894, 'Trainer/epoch': 0, 'Trainer/steps_train': 38}\n",
      "INFO 2025-02-10 20:15:29,881 train_utils.py: 271: Train Epoch: [1][ 0/38] | Batch Time: 35.22 (35.22) | Data Time: 31.54 (31.54) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 2.05e+00 (2.05e+00)\n",
      "INFO 2025-02-10 20:15:36,567 train_utils.py: 271: Train Epoch: [1][10/38] | Batch Time: 0.70 (3.81) | Data Time: 0.01 (2.87) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 5.02e+00 (3.69e+00)\n",
      "INFO 2025-02-10 20:15:42,455 train_utils.py: 271: Train Epoch: [1][20/38] | Batch Time: 0.41 (2.28) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.20e+00 (2.91e+00)\n",
      "INFO 2025-02-10 20:15:46,589 train_utils.py: 271: Train Epoch: [1][30/38] | Batch Time: 0.40 (1.68) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 2.74e+00 (2.55e+00)\n",
      "INFO 2025-02-10 20:16:00,177 trainer.py: 950: Estimated time remaining: 00d 00h 34m\n",
      "INFO 2025-02-10 20:16:00,178 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:16:00,178 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 2.7481477158634284, 'Losses/train_all_loss_mask': 0.05424891040954543, 'Losses/train_all_loss_dice': 1.0604112007116016, 'Losses/train_all_loss_iou': 0.6027582599536369, 'Losses/train_all_loss_class': 5.86943103031131e-08, 'Losses/train_all_core_loss': 2.7481477158634284, 'Trainer/where': 0.049342105263157895, 'Trainer/epoch': 1, 'Trainer/steps_train': 76}\n",
      "INFO 2025-02-10 20:16:36,666 train_utils.py: 271: Train Epoch: [2][ 0/38] | Batch Time: 35.22 (35.22) | Data Time: 31.48 (31.48) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 1.13e+00 (1.13e+00)\n",
      "INFO 2025-02-10 20:16:43,452 train_utils.py: 271: Train Epoch: [2][10/38] | Batch Time: 0.65 (3.82) | Data Time: 0.00 (2.87) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 7.89e-01 (2.78e+00)\n",
      "INFO 2025-02-10 20:16:49,480 train_utils.py: 271: Train Epoch: [2][20/38] | Batch Time: 0.41 (2.29) | Data Time: 0.00 (1.50) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.74e+00 (2.51e+00)\n",
      "INFO 2025-02-10 20:16:53,571 train_utils.py: 271: Train Epoch: [2][30/38] | Batch Time: 0.41 (1.68) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.33e+00 (2.28e+00)\n",
      "INFO 2025-02-10 20:17:07,013 trainer.py: 950: Estimated time remaining: 00d 00h 33m\n",
      "INFO 2025-02-10 20:17:07,014 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:17:07,014 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 2.099913173600247, 'Losses/train_all_loss_mask': 0.031962566953887675, 'Losses/train_all_loss_dice': 0.9582017515050737, 'Losses/train_all_loss_iou': 0.502460074856093, 'Losses/train_all_loss_class': 2.2629423246816867e-08, 'Losses/train_all_core_loss': 2.099913173600247, 'Trainer/where': 0.0743421052631579, 'Trainer/epoch': 2, 'Trainer/steps_train': 114}\n",
      "INFO 2025-02-10 20:17:43,428 train_utils.py: 271: Train Epoch: [3][ 0/38] | Batch Time: 35.17 (35.17) | Data Time: 31.65 (31.65) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 5.27e-01 (5.27e-01)\n",
      "INFO 2025-02-10 20:17:50,326 train_utils.py: 271: Train Epoch: [3][10/38] | Batch Time: 0.71 (3.82) | Data Time: 0.00 (2.88) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.07e+00 (1.59e+00)\n",
      "INFO 2025-02-10 20:17:56,227 train_utils.py: 271: Train Epoch: [3][20/38] | Batch Time: 0.42 (2.28) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 8.65e-01 (1.74e+00)\n",
      "INFO 2025-02-10 20:18:00,331 train_utils.py: 271: Train Epoch: [3][30/38] | Batch Time: 0.41 (1.68) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.01e+00 (1.52e+00)\n",
      "INFO 2025-02-10 20:18:13,722 trainer.py: 950: Estimated time remaining: 00d 00h 32m\n",
      "INFO 2025-02-10 20:18:13,723 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:18:13,723 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 2.007029282419305, 'Losses/train_all_loss_mask': 0.03904018639341781, 'Losses/train_all_loss_dice': 0.8346880705733049, 'Losses/train_all_loss_iou': 0.3915375020158918, 'Losses/train_all_loss_class': 2.8190374073101018e-08, 'Losses/train_all_core_loss': 2.007029282419305, 'Trainer/where': 0.0993421052631579, 'Trainer/epoch': 3, 'Trainer/steps_train': 152}\n",
      "INFO 2025-02-10 20:18:50,188 train_utils.py: 271: Train Epoch: [4][ 0/38] | Batch Time: 35.20 (35.20) | Data Time: 31.78 (31.78) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 2.50e+00 (2.50e+00)\n",
      "INFO 2025-02-10 20:18:57,025 train_utils.py: 271: Train Epoch: [4][10/38] | Batch Time: 0.64 (3.82) | Data Time: 0.01 (2.89) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 7.30e-01 (1.38e+00)\n",
      "INFO 2025-02-10 20:19:02,918 train_utils.py: 271: Train Epoch: [4][20/38] | Batch Time: 0.41 (2.28) | Data Time: 0.00 (1.52) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.64e+00 (1.31e+00)\n",
      "INFO 2025-02-10 20:19:06,939 train_utils.py: 271: Train Epoch: [4][30/38] | Batch Time: 0.39 (1.68) | Data Time: 0.00 (1.03) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.47e+00 (1.37e+00)\n",
      "INFO 2025-02-10 20:19:20,386 trainer.py: 950: Estimated time remaining: 00d 00h 31m\n",
      "INFO 2025-02-10 20:19:20,387 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:19:20,387 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.353060245513916, 'Losses/train_all_loss_mask': 0.01897220050082787, 'Losses/train_all_loss_dice': 0.6677224738033194, 'Losses/train_all_loss_iou': 0.30589372939185094, 'Losses/train_all_loss_class': 2.7038142136410526e-08, 'Losses/train_all_core_loss': 1.353060245513916, 'Trainer/where': 0.12434210526315789, 'Trainer/epoch': 4, 'Trainer/steps_train': 190}\n",
      "INFO 2025-02-10 20:19:57,054 train_utils.py: 271: Train Epoch: [5][ 0/38] | Batch Time: 35.41 (35.41) | Data Time: 31.49 (31.49) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 6.54e-01 (6.54e-01)\n",
      "INFO 2025-02-10 20:20:03,797 train_utils.py: 271: Train Epoch: [5][10/38] | Batch Time: 0.68 (3.83) | Data Time: 0.00 (2.87) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.81e+00 (1.11e+00)\n",
      "INFO 2025-02-10 20:20:09,819 train_utils.py: 271: Train Epoch: [5][20/38] | Batch Time: 0.41 (2.29) | Data Time: 0.00 (1.50) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.14e+00 (1.14e+00)\n",
      "INFO 2025-02-10 20:20:13,887 train_utils.py: 271: Train Epoch: [5][30/38] | Batch Time: 0.41 (1.69) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.55e-01 (1.42e+00)\n",
      "INFO 2025-02-10 20:20:27,503 trainer.py: 950: Estimated time remaining: 00d 00h 31m\n",
      "INFO 2025-02-10 20:20:27,504 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:20:27,504 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4573926298241866, 'Losses/train_all_loss_mask': 0.02274914189413386, 'Losses/train_all_loss_dice': 0.679875376193147, 'Losses/train_all_loss_iou': 0.32253440528323774, 'Losses/train_all_loss_class': 2.253092999835478e-08, 'Losses/train_all_core_loss': 1.4573926298241866, 'Trainer/where': 0.1493421052631579, 'Trainer/epoch': 5, 'Trainer/steps_train': 228}\n",
      "INFO 2025-02-10 20:21:04,391 train_utils.py: 271: Train Epoch: [6][ 0/38] | Batch Time: 35.52 (35.52) | Data Time: 31.74 (31.74) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 6.43e+00 (6.43e+00)\n",
      "INFO 2025-02-10 20:21:11,255 train_utils.py: 271: Train Epoch: [6][10/38] | Batch Time: 0.69 (3.85) | Data Time: 0.01 (2.89) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 3.09e+00 (1.95e+00)\n",
      "INFO 2025-02-10 20:21:17,273 train_utils.py: 271: Train Epoch: [6][20/38] | Batch Time: 0.42 (2.30) | Data Time: 0.00 (1.52) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.74e+00 (2.08e+00)\n",
      "INFO 2025-02-10 20:21:21,329 train_utils.py: 271: Train Epoch: [6][30/38] | Batch Time: 0.41 (1.69) | Data Time: 0.00 (1.03) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 3.08e-01 (2.16e+00)\n",
      "INFO 2025-02-10 20:21:34,751 trainer.py: 950: Estimated time remaining: 00d 00h 30m\n",
      "INFO 2025-02-10 20:21:34,752 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:21:34,752 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.9800858066270226, 'Losses/train_all_loss_mask': 0.03539967812646769, 'Losses/train_all_loss_dice': 0.848548872298316, 'Losses/train_all_loss_iou': 0.423543336556146, 'Losses/train_all_loss_class': 3.6780621381174e-08, 'Losses/train_all_core_loss': 1.9800858066270226, 'Trainer/where': 0.1743421052631579, 'Trainer/epoch': 6, 'Trainer/steps_train': 266}\n",
      "INFO 2025-02-10 20:22:11,100 train_utils.py: 271: Train Epoch: [7][ 0/38] | Batch Time: 35.11 (35.11) | Data Time: 31.44 (31.44) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 9.19e-01 (9.19e-01)\n",
      "INFO 2025-02-10 20:22:17,963 train_utils.py: 271: Train Epoch: [7][10/38] | Batch Time: 0.72 (3.82) | Data Time: 0.01 (2.86) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 6.52e-01 (1.35e+00)\n",
      "INFO 2025-02-10 20:22:24,023 train_utils.py: 271: Train Epoch: [7][20/38] | Batch Time: 0.40 (2.29) | Data Time: 0.00 (1.50) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 8.18e-01 (1.39e+00)\n",
      "INFO 2025-02-10 20:22:28,082 train_utils.py: 271: Train Epoch: [7][30/38] | Batch Time: 0.40 (1.68) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.08e+00 (1.32e+00)\n",
      "INFO 2025-02-10 20:22:41,550 trainer.py: 950: Estimated time remaining: 00d 00h 29m\n",
      "INFO 2025-02-10 20:22:41,551 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:22:41,551 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.286688913640223, 'Losses/train_all_loss_mask': 0.018186387152558093, 'Losses/train_all_loss_dice': 0.6510928665336809, 'Losses/train_all_loss_iou': 0.2718682783214669, 'Losses/train_all_loss_class': 2.337250492779944e-08, 'Losses/train_all_core_loss': 1.286688913640223, 'Trainer/where': 0.1993421052631579, 'Trainer/epoch': 7, 'Trainer/steps_train': 304}\n",
      "INFO 2025-02-10 20:23:18,024 train_utils.py: 271: Train Epoch: [8][ 0/38] | Batch Time: 35.22 (35.22) | Data Time: 31.57 (31.57) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 2.95e+00 (2.95e+00)\n",
      "INFO 2025-02-10 20:23:24,882 train_utils.py: 271: Train Epoch: [8][10/38] | Batch Time: 0.70 (3.83) | Data Time: 0.00 (2.87) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.41e+00 (1.21e+00)\n",
      "INFO 2025-02-10 20:23:30,954 train_utils.py: 271: Train Epoch: [8][20/38] | Batch Time: 0.41 (2.29) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 2.43e+00 (1.22e+00)\n",
      "INFO 2025-02-10 20:23:35,029 train_utils.py: 271: Train Epoch: [8][30/38] | Batch Time: 0.41 (1.68) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.63e+00 (1.52e+00)\n",
      "INFO 2025-02-10 20:23:48,428 trainer.py: 950: Estimated time remaining: 00d 00h 28m\n",
      "INFO 2025-02-10 20:23:48,428 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:23:48,428 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4953282867607318, 'Losses/train_all_loss_mask': 0.022960807901414994, 'Losses/train_all_loss_dice': 0.7147259492623178, 'Losses/train_all_loss_iou': 0.3213861738576701, 'Losses/train_all_loss_class': 2.9303583286873008e-08, 'Losses/train_all_core_loss': 1.4953282867607318, 'Trainer/where': 0.22434210526315787, 'Trainer/epoch': 8, 'Trainer/steps_train': 342}\n",
      "INFO 2025-02-10 20:24:24,967 train_utils.py: 271: Train Epoch: [9][ 0/38] | Batch Time: 35.28 (35.28) | Data Time: 31.91 (31.91) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 4.07e-01 (4.07e-01)\n",
      "INFO 2025-02-10 20:24:31,693 train_utils.py: 271: Train Epoch: [9][10/38] | Batch Time: 0.67 (3.82) | Data Time: 0.01 (2.91) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.23e+00 (1.23e+00)\n",
      "INFO 2025-02-10 20:24:37,704 train_utils.py: 271: Train Epoch: [9][20/38] | Batch Time: 0.41 (2.29) | Data Time: 0.00 (1.52) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.20e+00 (1.34e+00)\n",
      "INFO 2025-02-10 20:24:41,821 train_utils.py: 271: Train Epoch: [9][30/38] | Batch Time: 0.41 (1.68) | Data Time: 0.00 (1.03) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 8.79e-01 (1.29e+00)\n",
      "INFO 2025-02-10 20:24:55,295 trainer.py: 950: Estimated time remaining: 00d 00h 27m\n",
      "INFO 2025-02-10 20:24:55,295 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:24:55,295 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.3767966777086258, 'Losses/train_all_loss_mask': 0.01910269123771669, 'Losses/train_all_loss_dice': 0.6705100665751257, 'Losses/train_all_loss_iou': 0.32423274787633044, 'Losses/train_all_loss_class': 4.2595018595968866e-08, 'Losses/train_all_core_loss': 1.3767966777086258, 'Trainer/where': 0.24934210526315786, 'Trainer/epoch': 9, 'Trainer/steps_train': 380}\n",
      "INFO 2025-02-10 20:25:32,020 train_utils.py: 271: Train Epoch: [10][ 0/38] | Batch Time: 35.48 (35.48) | Data Time: 31.39 (31.39) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.66e+00 (1.66e+00)\n",
      "INFO 2025-02-10 20:25:38,745 train_utils.py: 271: Train Epoch: [10][10/38] | Batch Time: 0.63 (3.84) | Data Time: 0.00 (2.86) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.93e+00 (1.34e+00)\n",
      "INFO 2025-02-10 20:25:44,492 train_utils.py: 271: Train Epoch: [10][20/38] | Batch Time: 0.41 (2.28) | Data Time: 0.00 (1.50) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 7.15e-01 (1.28e+00)\n",
      "INFO 2025-02-10 20:25:48,491 train_utils.py: 271: Train Epoch: [10][30/38] | Batch Time: 0.39 (1.68) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.66e+00 (1.28e+00)\n",
      "INFO 2025-02-10 20:26:00,919 trainer.py: 950: Estimated time remaining: 00d 00h 26m\n",
      "INFO 2025-02-10 20:26:00,919 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:26:00,920 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.228948295900696, 'Losses/train_all_loss_mask': 0.016402377722490775, 'Losses/train_all_loss_dice': 0.6317603811621666, 'Losses/train_all_loss_iou': 0.26914030833071784, 'Losses/train_all_loss_class': 6.170085064838001e-08, 'Losses/train_all_core_loss': 1.228948295900696, 'Trainer/where': 0.2743421052631579, 'Trainer/epoch': 10, 'Trainer/steps_train': 418}\n",
      "INFO 2025-02-10 20:26:34,560 train_utils.py: 271: Train Epoch: [11][ 0/38] | Batch Time: 32.46 (32.46) | Data Time: 29.58 (29.58) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 6.86e-01 (6.86e-01)\n",
      "INFO 2025-02-10 20:26:41,358 train_utils.py: 271: Train Epoch: [11][10/38] | Batch Time: 0.70 (3.57) | Data Time: 0.01 (2.69) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 9.60e-01 (1.29e+00)\n",
      "INFO 2025-02-10 20:26:47,378 train_utils.py: 271: Train Epoch: [11][20/38] | Batch Time: 0.40 (2.16) | Data Time: 0.00 (1.41) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 3.08e+00 (1.64e+00)\n",
      "INFO 2025-02-10 20:26:51,549 train_utils.py: 271: Train Epoch: [11][30/38] | Batch Time: 0.42 (1.60) | Data Time: 0.00 (0.96) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 1.08e+00 (1.53e+00)\n",
      "INFO 2025-02-10 20:27:04,930 trainer.py: 950: Estimated time remaining: 00d 00h 24m\n",
      "INFO 2025-02-10 20:27:04,931 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:27:04,931 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4889371748033322, 'Losses/train_all_loss_mask': 0.020597388459320524, 'Losses/train_all_loss_dice': 0.7363324279063627, 'Losses/train_all_loss_iou': 0.3406569440113871, 'Losses/train_all_loss_class': 3.7843964456132774e-08, 'Losses/train_all_core_loss': 1.4889371748033322, 'Trainer/where': 0.29934210526315785, 'Trainer/epoch': 11, 'Trainer/steps_train': 456}\n",
      "INFO 2025-02-10 20:27:41,239 train_utils.py: 271: Train Epoch: [12][ 0/38] | Batch Time: 35.05 (35.05) | Data Time: 32.10 (32.10) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 5.43e-01 (5.43e-01)\n",
      "INFO 2025-02-10 20:27:47,879 train_utils.py: 271: Train Epoch: [12][10/38] | Batch Time: 0.72 (3.79) | Data Time: 0.01 (2.92) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 8.61e-01 (9.40e-01)\n",
      "INFO 2025-02-10 20:27:53,746 train_utils.py: 271: Train Epoch: [12][20/38] | Batch Time: 0.41 (2.26) | Data Time: 0.00 (1.53) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.27e-01 (1.05e+00)\n",
      "INFO 2025-02-10 20:27:57,845 train_utils.py: 271: Train Epoch: [12][30/38] | Batch Time: 0.42 (1.67) | Data Time: 0.00 (1.04) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.10e+00 (1.15e+00)\n",
      "INFO 2025-02-10 20:28:11,296 trainer.py: 950: Estimated time remaining: 00d 00h 24m\n",
      "INFO 2025-02-10 20:28:11,296 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:28:11,296 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.1370904469176342, 'Losses/train_all_loss_mask': 0.015637740566346207, 'Losses/train_all_loss_dice': 0.5744631780605567, 'Losses/train_all_loss_iou': 0.24987242027725043, 'Losses/train_all_loss_class': 3.8720618190934974e-08, 'Losses/train_all_core_loss': 1.1370904469176342, 'Trainer/where': 0.3243421052631579, 'Trainer/epoch': 12, 'Trainer/steps_train': 494}\n",
      "INFO 2025-02-10 20:28:47,744 train_utils.py: 271: Train Epoch: [13][ 0/38] | Batch Time: 35.16 (35.16) | Data Time: 32.12 (32.12) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 4.91e-01 (4.91e-01)\n",
      "INFO 2025-02-10 20:28:54,529 train_utils.py: 271: Train Epoch: [13][10/38] | Batch Time: 0.70 (3.81) | Data Time: 0.01 (2.92) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 2.35e+00 (1.56e+00)\n",
      "INFO 2025-02-10 20:29:00,439 train_utils.py: 271: Train Epoch: [13][20/38] | Batch Time: 0.41 (2.28) | Data Time: 0.00 (1.53) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 8.25e-01 (1.41e+00)\n",
      "INFO 2025-02-10 20:29:04,538 train_utils.py: 271: Train Epoch: [13][30/38] | Batch Time: 0.41 (1.68) | Data Time: 0.00 (1.04) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 5.30e+00 (1.39e+00)\n",
      "INFO 2025-02-10 20:29:17,959 trainer.py: 950: Estimated time remaining: 00d 00h 23m\n",
      "INFO 2025-02-10 20:29:17,959 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:29:17,959 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4190233977217424, 'Losses/train_all_loss_mask': 0.02106813895883725, 'Losses/train_all_loss_dice': 0.7024935919203257, 'Losses/train_all_loss_iou': 0.29516689910700444, 'Losses/train_all_loss_class': 1.297359867770336e-07, 'Losses/train_all_core_loss': 1.4190233977217424, 'Trainer/where': 0.3493421052631579, 'Trainer/epoch': 13, 'Trainer/steps_train': 532}\n",
      "INFO 2025-02-10 20:29:54,311 train_utils.py: 271: Train Epoch: [14][ 0/38] | Batch Time: 35.06 (35.06) | Data Time: 31.67 (31.67) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.47e+00 (1.47e+00)\n",
      "INFO 2025-02-10 20:30:01,150 train_utils.py: 271: Train Epoch: [14][10/38] | Batch Time: 0.70 (3.81) | Data Time: 0.00 (2.88) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.62e+00 (1.28e+00)\n",
      "INFO 2025-02-10 20:30:06,990 train_utils.py: 271: Train Epoch: [14][20/38] | Batch Time: 0.40 (2.27) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 8.57e-01 (1.18e+00)\n",
      "INFO 2025-02-10 20:30:11,102 train_utils.py: 271: Train Epoch: [14][30/38] | Batch Time: 0.41 (1.67) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 8.70e-01 (1.37e+00)\n",
      "INFO 2025-02-10 20:30:24,481 trainer.py: 950: Estimated time remaining: 00d 00h 22m\n",
      "INFO 2025-02-10 20:30:24,481 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:30:24,482 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.3849427151052576, 'Losses/train_all_loss_mask': 0.019802414579316974, 'Losses/train_all_loss_dice': 0.6819825650830018, 'Losses/train_all_loss_iou': 0.3069116902586661, 'Losses/train_all_loss_class': 1.907729610862777e-07, 'Losses/train_all_core_loss': 1.3849427151052576, 'Trainer/where': 0.37434210526315786, 'Trainer/epoch': 14, 'Trainer/steps_train': 570}\n",
      "INFO 2025-02-10 20:31:00,978 train_utils.py: 271: Train Epoch: [15][ 0/38] | Batch Time: 35.23 (35.23) | Data Time: 32.06 (32.06) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 5.31e-01 (5.31e-01)\n",
      "INFO 2025-02-10 20:31:07,948 train_utils.py: 271: Train Epoch: [15][10/38] | Batch Time: 0.67 (3.84) | Data Time: 0.01 (2.92) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.05e+00 (1.17e+00)\n",
      "INFO 2025-02-10 20:31:13,900 train_utils.py: 271: Train Epoch: [15][20/38] | Batch Time: 0.41 (2.29) | Data Time: 0.00 (1.53) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 7.12e-01 (1.13e+00)\n",
      "INFO 2025-02-10 20:31:18,288 train_utils.py: 271: Train Epoch: [15][30/38] | Batch Time: 0.43 (1.70) | Data Time: 0.00 (1.04) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.88e+00 (1.18e+00)\n",
      "INFO 2025-02-10 20:31:31,053 trainer.py: 950: Estimated time remaining: 00d 00h 22m\n",
      "INFO 2025-02-10 20:31:31,054 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:31:31,054 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2404004194234546, 'Losses/train_all_loss_mask': 0.017086282495017115, 'Losses/train_all_loss_dice': 0.6206481166576084, 'Losses/train_all_loss_iou': 0.27802659316282524, 'Losses/train_all_loss_class': 7.146094655018014e-08, 'Losses/train_all_core_loss': 1.2404004194234546, 'Trainer/where': 0.3993421052631579, 'Trainer/epoch': 15, 'Trainer/steps_train': 608}\n",
      "INFO 2025-02-10 20:32:05,370 train_utils.py: 271: Train Epoch: [16][ 0/38] | Batch Time: 33.11 (33.11) | Data Time: 30.00 (30.00) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 1.44e+00 (1.44e+00)\n",
      "INFO 2025-02-10 20:32:12,070 train_utils.py: 271: Train Epoch: [16][10/38] | Batch Time: 0.64 (3.62) | Data Time: 0.01 (2.73) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 5.35e-01 (1.31e+00)\n",
      "INFO 2025-02-10 20:32:17,984 train_utils.py: 271: Train Epoch: [16][20/38] | Batch Time: 0.40 (2.18) | Data Time: 0.00 (1.43) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 6.94e-01 (1.18e+00)\n",
      "INFO 2025-02-10 20:32:21,980 train_utils.py: 271: Train Epoch: [16][30/38] | Batch Time: 0.41 (1.60) | Data Time: 0.00 (0.97) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 5.59e-01 (1.19e+00)\n",
      "INFO 2025-02-10 20:32:35,992 trainer.py: 950: Estimated time remaining: 00d 00h 20m\n",
      "INFO 2025-02-10 20:32:35,992 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:32:35,992 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.1216893447072882, 'Losses/train_all_loss_mask': 0.013311084500808073, 'Losses/train_all_loss_dice': 0.6084486728436068, 'Losses/train_all_loss_iou': 0.24701890623883196, 'Losses/train_all_loss_class': 6.071943373171284e-08, 'Losses/train_all_core_loss': 1.1216893447072882, 'Trainer/where': 0.42434210526315785, 'Trainer/epoch': 16, 'Trainer/steps_train': 646}\n",
      "INFO 2025-02-10 20:33:10,293 train_utils.py: 271: Train Epoch: [17][ 0/38] | Batch Time: 33.11 (33.11) | Data Time: 30.45 (30.45) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 1.62e+00 (1.62e+00)\n",
      "INFO 2025-02-10 20:33:16,749 train_utils.py: 271: Train Epoch: [17][10/38] | Batch Time: 0.69 (3.60) | Data Time: 0.00 (2.77) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.43e-01 (1.07e+00)\n",
      "INFO 2025-02-10 20:33:23,013 train_utils.py: 271: Train Epoch: [17][20/38] | Batch Time: 0.43 (2.18) | Data Time: 0.00 (1.45) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 4.25e-01 (1.18e+00)\n",
      "INFO 2025-02-10 20:33:27,224 train_utils.py: 271: Train Epoch: [17][30/38] | Batch Time: 0.41 (1.61) | Data Time: 0.00 (0.99) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 1.51e+00 (1.32e+00)\n",
      "INFO 2025-02-10 20:33:40,641 trainer.py: 950: Estimated time remaining: 00d 00h 19m\n",
      "INFO 2025-02-10 20:33:40,641 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:33:40,641 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2233299856123172, 'Losses/train_all_loss_mask': 0.017055650076240693, 'Losses/train_all_loss_dice': 0.6035952728829885, 'Losses/train_all_loss_iou': 0.27862166181990977, 'Losses/train_all_loss_class': 6.0418563777762e-08, 'Losses/train_all_core_loss': 1.2233299856123172, 'Trainer/where': 0.4493421052631579, 'Trainer/epoch': 17, 'Trainer/steps_train': 684}\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "INFO 2025-02-10 20:34:17,232 train_utils.py: 271: Train Epoch: [18][ 0/38] | Batch Time: 35.31 (35.31) | Data Time: 31.93 (31.93) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 5.71e-01 (5.71e-01)\n",
      "INFO 2025-02-10 20:34:24,055 train_utils.py: 271: Train Epoch: [18][10/38] | Batch Time: 0.66 (3.83) | Data Time: 0.01 (2.91) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 8.49e-01 (1.14e+00)\n",
      "INFO 2025-02-10 20:34:29,939 train_utils.py: 271: Train Epoch: [18][20/38] | Batch Time: 0.41 (2.29) | Data Time: 0.00 (1.52) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 8.09e-01 (1.35e+00)\n",
      "INFO 2025-02-10 20:34:34,026 train_utils.py: 271: Train Epoch: [18][30/38] | Batch Time: 0.41 (1.68) | Data Time: 0.00 (1.03) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 5.88e-01 (1.18e+00)\n",
      "INFO 2025-02-10 20:34:47,412 trainer.py: 950: Estimated time remaining: 00d 00h 19m\n",
      "INFO 2025-02-10 20:34:47,413 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:34:47,413 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.1577381358334893, 'Losses/train_all_loss_mask': 0.013740751377649997, 'Losses/train_all_loss_dice': 0.6201575818030458, 'Losses/train_all_loss_iou': 0.26276543677637454, 'Losses/train_all_loss_class': 8.8359562797909e-08, 'Losses/train_all_core_loss': 1.1577381358334893, 'Trainer/where': 0.4743421052631579, 'Trainer/epoch': 18, 'Trainer/steps_train': 722}\n",
      "INFO 2025-02-10 20:35:23,858 train_utils.py: 271: Train Epoch: [19][ 0/38] | Batch Time: 35.20 (35.20) | Data Time: 31.78 (31.78) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.23e+00 (1.23e+00)\n",
      "INFO 2025-02-10 20:35:30,711 train_utils.py: 271: Train Epoch: [19][10/38] | Batch Time: 0.71 (3.82) | Data Time: 0.01 (2.89) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.28e+00 (7.17e-01)\n",
      "INFO 2025-02-10 20:35:36,651 train_utils.py: 271: Train Epoch: [19][20/38] | Batch Time: 0.42 (2.29) | Data Time: 0.00 (1.52) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.17e+00 (8.49e-01)\n",
      "INFO 2025-02-10 20:35:40,770 train_utils.py: 271: Train Epoch: [19][30/38] | Batch Time: 0.42 (1.68) | Data Time: 0.00 (1.03) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 8.26e-01 (9.35e-01)\n",
      "INFO 2025-02-10 20:35:54,189 trainer.py: 950: Estimated time remaining: 00d 00h 18m\n",
      "INFO 2025-02-10 20:35:54,190 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:35:54,190 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.9337207494597686, 'Losses/train_all_loss_mask': 0.0114705684643827, 'Losses/train_all_loss_dice': 0.5077623223003588, 'Losses/train_all_loss_iou': 0.1965470298340446, 'Losses/train_all_loss_class': 2.5135363961621764e-08, 'Losses/train_all_core_loss': 0.9337207494597686, 'Trainer/where': 0.49934210526315786, 'Trainer/epoch': 19, 'Trainer/steps_train': 760}\n",
      "INFO 2025-02-10 20:36:30,550 train_utils.py: 271: Train Epoch: [20][ 0/38] | Batch Time: 35.08 (35.08) | Data Time: 31.54 (31.54) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 1.40e+00 (1.40e+00)\n",
      "INFO 2025-02-10 20:36:37,310 train_utils.py: 271: Train Epoch: [20][10/38] | Batch Time: 0.70 (3.80) | Data Time: 0.00 (2.87) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 1.94e+00 (1.44e+00)\n",
      "INFO 2025-02-10 20:36:43,244 train_utils.py: 271: Train Epoch: [20][20/38] | Batch Time: 0.39 (2.27) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 8.29e-01 (1.23e+00)\n",
      "INFO 2025-02-10 20:36:47,323 train_utils.py: 271: Train Epoch: [20][30/38] | Batch Time: 0.42 (1.67) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 3.84e+00 (1.31e+00)\n",
      "INFO 2025-02-10 20:37:00,569 trainer.py: 950: Estimated time remaining: 00d 00h 17m\n",
      "INFO 2025-02-10 20:37:00,569 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:37:00,569 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2321792048843283, 'Losses/train_all_loss_mask': 0.016779099205067677, 'Losses/train_all_loss_dice': 0.6200239740704235, 'Losses/train_all_loss_iou': 0.27657319909255756, 'Losses/train_all_loss_class': 5.986329087225529e-08, 'Losses/train_all_core_loss': 1.2321792048843283, 'Trainer/where': 0.5243421052631578, 'Trainer/epoch': 20, 'Trainer/steps_train': 798}\n",
      "INFO 2025-02-10 20:37:36,663 train_utils.py: 271: Train Epoch: [21][ 0/38] | Batch Time: 34.84 (34.84) | Data Time: 31.56 (31.56) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 3.11e-01 (3.11e-01)\n",
      "INFO 2025-02-10 20:37:43,329 train_utils.py: 271: Train Epoch: [21][10/38] | Batch Time: 0.64 (3.77) | Data Time: 0.01 (2.87) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.76e-01 (9.04e-01)\n",
      "INFO 2025-02-10 20:37:49,284 train_utils.py: 271: Train Epoch: [21][20/38] | Batch Time: 0.40 (2.26) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.76e+00 (1.07e+00)\n",
      "INFO 2025-02-10 20:37:53,361 train_utils.py: 271: Train Epoch: [21][30/38] | Batch Time: 0.40 (1.66) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 6.79e-01 (1.16e+00)\n",
      "INFO 2025-02-10 20:38:06,677 trainer.py: 950: Estimated time remaining: 00d 00h 16m\n",
      "INFO 2025-02-10 20:38:06,678 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:38:06,678 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2008403092622757, 'Losses/train_all_loss_mask': 0.015554035713553037, 'Losses/train_all_loss_dice': 0.6182413473725319, 'Losses/train_all_loss_iou': 0.2715181680886369, 'Losses/train_all_loss_class': 7.314083522936598e-08, 'Losses/train_all_core_loss': 1.2008403092622757, 'Trainer/where': 0.5493421052631579, 'Trainer/epoch': 21, 'Trainer/steps_train': 836}\n",
      "INFO 2025-02-10 20:38:42,688 train_utils.py: 271: Train Epoch: [22][ 0/38] | Batch Time: 34.75 (34.75) | Data Time: 31.62 (31.62) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 4.05e-01 (4.05e-01)\n",
      "INFO 2025-02-10 20:38:49,481 train_utils.py: 271: Train Epoch: [22][10/38] | Batch Time: 0.67 (3.78) | Data Time: 0.01 (2.88) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 1.95e+00 (1.09e+00)\n",
      "INFO 2025-02-10 20:38:55,328 train_utils.py: 271: Train Epoch: [22][20/38] | Batch Time: 0.40 (2.26) | Data Time: 0.00 (1.51) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 1.69e+00 (1.07e+00)\n",
      "INFO 2025-02-10 20:38:59,346 train_utils.py: 271: Train Epoch: [22][30/38] | Batch Time: 0.40 (1.66) | Data Time: 0.00 (1.02) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 4.67e-01 (1.23e+00)\n",
      "INFO 2025-02-10 20:39:12,607 trainer.py: 950: Estimated time remaining: 00d 00h 15m\n",
      "INFO 2025-02-10 20:39:12,608 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:39:12,608 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.3741563833073567, 'Losses/train_all_loss_mask': 0.021428891132879806, 'Losses/train_all_loss_dice': 0.6562647988137446, 'Losses/train_all_loss_iou': 0.2893136453471686, 'Losses/train_all_loss_class': 1.3352472692795825e-07, 'Losses/train_all_core_loss': 1.3741563833073567, 'Trainer/where': 0.5743421052631579, 'Trainer/epoch': 22, 'Trainer/steps_train': 874}\n",
      "INFO 2025-02-10 20:39:48,669 train_utils.py: 271: Train Epoch: [23][ 0/38] | Batch Time: 34.81 (34.81) | Data Time: 31.33 (31.33) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 8.38e-01 (8.38e-01)\n",
      "INFO 2025-02-10 20:39:55,362 train_utils.py: 271: Train Epoch: [23][10/38] | Batch Time: 0.69 (3.77) | Data Time: 0.01 (2.85) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 1.06e+00 (1.06e+00)\n",
      "INFO 2025-02-10 20:40:01,302 train_utils.py: 271: Train Epoch: [23][20/38] | Batch Time: 0.40 (2.26) | Data Time: 0.00 (1.50) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 6.03e-01 (1.25e+00)\n",
      "INFO 2025-02-10 20:40:05,437 train_utils.py: 271: Train Epoch: [23][30/38] | Batch Time: 0.41 (1.66) | Data Time: 0.00 (1.01) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 8.23e-01 (1.21e+00)\n",
      "INFO 2025-02-10 20:40:18,698 trainer.py: 950: Estimated time remaining: 00d 00h 14m\n",
      "INFO 2025-02-10 20:40:18,698 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:40:18,698 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.1586875499863374, 'Losses/train_all_loss_mask': 0.014044470631664521, 'Losses/train_all_loss_dice': 0.6273938864469528, 'Losses/train_all_loss_iou': 0.25040415037227304, 'Losses/train_all_loss_class': 8.94373525312841e-08, 'Losses/train_all_core_loss': 1.1586875499863374, 'Trainer/where': 0.5993421052631579, 'Trainer/epoch': 23, 'Trainer/steps_train': 912}\n",
      "INFO 2025-02-10 20:40:54,782 train_utils.py: 271: Train Epoch: [24][ 0/38] | Batch Time: 34.82 (34.82) | Data Time: 31.11 (31.11) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.31e+00 (1.31e+00)\n",
      "INFO 2025-02-10 20:41:01,465 train_utils.py: 271: Train Epoch: [24][10/38] | Batch Time: 0.64 (3.77) | Data Time: 0.01 (2.83) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.94e+00 (1.53e+00)\n",
      "INFO 2025-02-10 20:41:07,413 train_utils.py: 271: Train Epoch: [24][20/38] | Batch Time: 0.40 (2.26) | Data Time: 0.00 (1.49) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 3.44e+00 (1.44e+00)\n",
      "INFO 2025-02-10 20:41:11,734 train_utils.py: 271: Train Epoch: [24][30/38] | Batch Time: 0.55 (1.67) | Data Time: 0.00 (1.01) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.02e+00 (1.31e+00)\n",
      "INFO 2025-02-10 20:41:26,132 trainer.py: 950: Estimated time remaining: 00d 00h 13m\n",
      "INFO 2025-02-10 20:41:26,132 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-10 20:41:26,132 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2041662139327902, 'Losses/train_all_loss_mask': 0.014210415575163145, 'Losses/train_all_loss_dice': 0.6338684696115946, 'Losses/train_all_loss_iou': 0.28608914445105355, 'Losses/train_all_loss_class': 2.8946654080597885e-07, 'Losses/train_all_core_loss': 1.2041662139327902, 'Trainer/where': 0.6243421052631579, 'Trainer/epoch': 24, 'Trainer/steps_train': 950}\n",
      "INFO 2025-02-10 20:42:03,719 train_utils.py: 271: Train Epoch: [25][ 0/38] | Batch Time: 36.33 (36.33) | Data Time: 32.81 (32.81) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 7.07e-01 (7.07e-01)\n",
      "INFO 2025-02-10 20:42:10,709 train_utils.py: 271: Train Epoch: [25][10/38] | Batch Time: 0.67 (3.94) | Data Time: 0.01 (2.99) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 4.40e-01 (7.07e-01)\n",
      "INFO 2025-02-10 20:42:16,670 train_utils.py: 271: Train Epoch: [25][20/38] | Batch Time: 0.43 (2.35) | Data Time: 0.00 (1.57) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 6.28e-01 (1.08e+00)\n",
      "INFO 2025-02-10 20:42:20,828 train_utils.py: 271: Train Epoch: [25][30/38] | Batch Time: 0.42 (1.72) | Data Time: 0.00 (1.06) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 9.33e-01 (1.10e+00)\n"
     ]
    }
   ],
   "source": [
    "# estamos en la carpeta Ej_sam2/sam2\n",
    "!python training/train.py -c 'configs/train.yaml' --use-cluster 0 --num-gpus 1\n",
    "\n",
    "# model weights saved in: /sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t\t    docker-compose.yaml  pyproject.toml    sav_dataset\n",
      "backend.Dockerfile  INSTALL.md\t\t README.md\t   setup.py\n",
      "checkpoints\t    LICENSE\t\t RELEASE_NOTES.md  tools\n",
      "CODE_OF_CONDUCT.md  LICENSE_cctorch\t sam2\t\t   training\n",
      "CONTRIBUTING.md     MANIFEST.in\t\t SAM_2.egg-info\n",
      "demo\t\t    notebooks\t\t sam2_logs\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supervision in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: contourpy>=1.0.7 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (1.3.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (4.10.0)\n",
      "Requirement already satisfied: pillow>=9.4 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (2.32.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from supervision) (4.67.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /home/iabd/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/sam2.1/sam2.1_hiera_b+.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m sam2 \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_sam2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m mask_generator \u001b[38;5;241m=\u001b[39m SAM2AutomaticMaskGenerator(sam2)\n\u001b[1;32m     25\u001b[0m checkpoint_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/sam2/sam2/checkpoints/sam2.1_hiera_base_plus.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Escritorio/clase/iabd/ia/sapa/deep_learning/Ej10/sam2/sam2/build_sam.py:93\u001b[0m, in \u001b[0;36mbuild_sam2\u001b[0;34m(config_file, ckpt_path, device, mode, hydra_overrides_extra, apply_postprocessing, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m OmegaConf\u001b[38;5;241m.\u001b[39mresolve(cfg)\n\u001b[1;32m     92\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mmodel, _recursive_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 93\u001b[0m \u001b[43m_load_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Escritorio/clase/iabd/ia/sapa/deep_learning/Ej10/sam2/sam2/build_sam.py:166\u001b[0m, in \u001b[0;36m_load_checkpoint\u001b[0;34m(model, ckpt_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_checkpoint\u001b[39m(model, ckpt_path):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckpt_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m         sd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    167\u001b[0m         missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(sd)\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n",
      "File \u001b[0;32m~/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages/ultralytics/utils/patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/iabd_3_10_SAM2/lib/python3.10/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt'"
     ]
    }
   ],
   "source": [
    "# ya esta instalado mas arriba\n",
    "!pip install supervision    \n",
    "\n",
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "import supervision as sv\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# use bfloat16 for inference\n",
    "# from Meta notebook\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "\ttorch.backends.cuda.matmul.allow_tf32 = True\n",
    "\ttorch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "checkpoint = \"/sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\")\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "\n",
    "checkpoint_base = \"/sam2/sam2/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "model_cfg_base = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "sam2_base = build_sam2(model_cfg_base, checkpoint_base, device=\"cuda\")\n",
    "mask_generator_base = SAM2AutomaticMaskGenerator(sam2_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_set = os.listdir(\"/content/data/valid\")\n",
    "\n",
    "# choose random with .json extension\n",
    "image = random.choice([img for img in validation_set if img.endswith(\".jpg\")])\n",
    "image = os.path.join(\"/content/data/valid\", image)\n",
    "opened_image = np.array(Image.open(image).convert(\"RGB\"))\n",
    "result = mask_generator.generate(opened_image)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=result)\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\n",
    "annotated_image = opened_image.copy()\n",
    "annotated_image = mask_annotator.annotate(annotated_image, detections=detections)\n",
    "\n",
    "base_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\n",
    "\n",
    "base_result = mask_generator_base.generate(opened_image)\n",
    "base_detections = sv.Detections.from_sam(sam_result=base_result)\n",
    "base_annotated_image = opened_image.copy()\n",
    "base_annotated_image = base_annotator.annotate(base_annotated_image, detections=base_detections)\n",
    "\n",
    "sv.plot_images_grid(images=[annotated_image, base_annotated_image], grid_size=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 11 (OPCIONAL)\n",
    "Haz fine tuning con sam2 o yolo11 para segmentación con otro dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 12 (OPCIONAL)\n",
    "En el siguiente enlace encontraras diferentes modelos y trabajos sobre la visión por computación, prueba alguno de ellos (algunos que no funcionan porque las librerías se han actualizado y tienes que adaptarlos, otros son recientes y no deberías tener problemas).\n",
    "\n",
    "https://github.com/roboflow/notebooks\n",
    "\n",
    "ELIMINA LOS COMENTARIOS QUE HAY EN EL CÓDIGO Y COMENTA EL CÓDIGO CON TUS PROPIAS PALABRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "14_deep_computer_vision_with_cnns.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iabd_3_10_SAM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0401482a18a94f22b95d5321bfa6f414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c08c78c0d484eed9638ad2b757ab584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2839afc6cb6d4a50b0bdad1fcb7f39d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eefd1a01ef1c46e09ffbd97ad25377cf",
       "IPY_MODEL_d142189db76a4681a22f38ae252e4ebc",
       "IPY_MODEL_d441368305704ab9a3bdbe762ab340a4"
      ],
      "layout": "IPY_MODEL_1c08c78c0d484eed9638ad2b757ab584"
     }
    },
    "54a90429726b4d848358cafae87ad893": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57cbb645792f45adbfab9b29aa708809": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f0660be3bf44dd48fd42cd52a507e32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b681dc2200ad4ee397a46602e8f4f654": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d142189db76a4681a22f38ae252e4ebc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54a90429726b4d848358cafae87ad893",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0401482a18a94f22b95d5321bfa6f414",
      "value": 5
     }
    },
    "d441368305704ab9a3bdbe762ab340a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8ef3c06db574e3f88dc9a8c0bcd22ab",
      "placeholder": "​",
      "style": "IPY_MODEL_8f0660be3bf44dd48fd42cd52a507e32",
      "value": " 5/5 [00:10&lt;00:00,  2.12s/ file]"
     }
    },
    "eefd1a01ef1c46e09ffbd97ad25377cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b681dc2200ad4ee397a46602e8f4f654",
      "placeholder": "​",
      "style": "IPY_MODEL_57cbb645792f45adbfab9b29aa708809",
      "value": "Dl Completed...: 100%"
     }
    },
    "f8ef3c06db574e3f88dc9a8c0bcd22ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
