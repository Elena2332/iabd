{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 20:06:10.242769: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-07 20:06:10.245796: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-07 20:06:10.256417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736276770.273683   97228 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736276770.278448   97228 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 20:06:10.297111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántas neuronas hacen falta en la capa de salida para clasificar emails entre correo no deseado y correo deseado? \n",
    " - 1\n",
    "\n",
    "¿Qué función de activación deberías usar en la capa de salida? \n",
    " - Sigmoide\n",
    "\n",
    "Si, en cambio, quieres enfrentarte a MNIST, ¿cuántas neuronas necesitas en la capa de salida y qué función de activación deberías usar? \n",
    " - 10 neuronas (1 neurona por clase), funcion de activacion: Softmax\n",
    "\n",
    "¿Y si quieres que la red prediga los precios de la vivienda en California?\n",
    " - 1 neurona, funcion de activacion: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena un red neuronal profunda con el conjunto de datos MNIST (puedes cargarlo con tf. keras. datasets.minst.load_data()). Intenta conseguir la mejor pruebas en el conjunto de validación  ajustando los hiperparámetros de modo manual. Prueba a buscar una buena tasa de aprendizaje y utiliza la detención temprana (mínimo hay que conseguir un 97%).\n",
    "\n",
    "Pasos a seguir: \n",
    "+ Entrena varios modelos con distintas tasas de aprendizaje\n",
    "+ Guarda cada modelo en un fichero\n",
    "+ Carga cada modelo y evalúalo en el conjunto de cargas y quedate con programación con el mejor (bucle que se quede con el mejor), mostrando la tasa de aprendizaje y la exactitud del mejor modelo.\n",
    "\n",
    "Si no obtienes más de un 97% vuelve a empezar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener, separar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "\n",
    "#Dividimos los datos completos de entrenamiento en datos de entrenamiento y validación\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar varios modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 20:06:13.361783: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# 1 entrada, 2 ocultas y 1 salida de 10 neuronas(1 por clase)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(124, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8025 - loss: 5.2708 - val_accuracy: 0.9232 - val_loss: 0.2999\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9155 - loss: 0.3467 - val_accuracy: 0.9418 - val_loss: 0.2180\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9411 - loss: 0.2262 - val_accuracy: 0.9594 - val_loss: 0.1480\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9504 - loss: 0.1759 - val_accuracy: 0.9522 - val_loss: 0.1804\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1475 - val_accuracy: 0.9580 - val_loss: 0.1555\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1322 - val_accuracy: 0.9610 - val_loss: 0.1381\n",
      "Epoch 7/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1180 - val_accuracy: 0.9638 - val_loss: 0.1490\n",
      "Epoch 8/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.1023 - val_accuracy: 0.9716 - val_loss: 0.1230\n",
      "Epoch 9/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0981 - val_accuracy: 0.9676 - val_loss: 0.1290\n",
      "Epoch 10/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0917 - val_accuracy: 0.9696 - val_loss: 0.1337\n",
      "Epoch 11/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0903 - val_accuracy: 0.9692 - val_loss: 0.1371\n",
      "Epoch 12/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0731 - val_accuracy: 0.9646 - val_loss: 0.1613\n",
      "Epoch 13/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0690 - val_accuracy: 0.9740 - val_loss: 0.1222\n",
      "Epoch 14/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0658 - val_accuracy: 0.9714 - val_loss: 0.1392\n",
      "Epoch 15/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0710 - val_accuracy: 0.9728 - val_loss: 0.1391\n",
      "Epoch 16/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9838 - loss: 0.0635 - val_accuracy: 0.9724 - val_loss: 0.1264\n",
      "Epoch 17/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0596 - val_accuracy: 0.9720 - val_loss: 0.1449\n",
      "Epoch 18/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0561 - val_accuracy: 0.9696 - val_loss: 0.1570\n",
      "\n",
      "\n",
      "\n",
      "Learning rate:  0.003\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9360 - loss: 0.3087 - val_accuracy: 0.9524 - val_loss: 0.2821\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9401 - loss: 0.2667 - val_accuracy: 0.9498 - val_loss: 0.2739\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9456 - loss: 0.2488 - val_accuracy: 0.9468 - val_loss: 0.2815\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9416 - loss: 0.2635 - val_accuracy: 0.9604 - val_loss: 0.2427\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.2338 - val_accuracy: 0.9360 - val_loss: 0.2966\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9480 - loss: 0.2393 - val_accuracy: 0.9506 - val_loss: 0.2408\n",
      "Epoch 7/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9526 - loss: 0.2157 - val_accuracy: 0.9568 - val_loss: 0.3167\n",
      "Epoch 8/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.2289 - val_accuracy: 0.9490 - val_loss: 0.3581\n",
      "Epoch 9/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.2278 - val_accuracy: 0.9516 - val_loss: 0.2776\n",
      "Epoch 10/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9564 - loss: 0.2168 - val_accuracy: 0.9558 - val_loss: 0.2522\n",
      "Epoch 11/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.2113 - val_accuracy: 0.9508 - val_loss: 0.2604\n",
      "\n",
      "\n",
      "\n",
      "Learning rate:  0.005\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8997 - loss: 0.5527 - val_accuracy: 0.9088 - val_loss: 0.5264\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8774 - loss: 0.5551 - val_accuracy: 0.6858 - val_loss: 1.1160\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8189 - loss: 0.7897 - val_accuracy: 0.8374 - val_loss: 0.7414\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8389 - loss: 0.6445 - val_accuracy: 0.8108 - val_loss: 0.7440\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.7830 - val_accuracy: 0.9014 - val_loss: 0.4748\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8443 - loss: 0.7049 - val_accuracy: 0.8088 - val_loss: 0.9226\n",
      "Epoch 7/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8151 - loss: 0.8270 - val_accuracy: 0.8508 - val_loss: 0.8088\n",
      "Epoch 8/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8313 - loss: 0.7240 - val_accuracy: 0.8766 - val_loss: 0.8528\n",
      "Epoch 9/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8205 - loss: 0.8577 - val_accuracy: 0.8422 - val_loss: 1.0836\n",
      "Epoch 10/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8427 - loss: 0.7072 - val_accuracy: 0.8556 - val_loss: 1.1649\n",
      "\n",
      "\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5040 - loss: 1.8408 - val_accuracy: 0.2296 - val_loss: 1.9626\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1766 - loss: 2.3357 - val_accuracy: 0.1444 - val_loss: 2.2328\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1332 - loss: 2.3102 - val_accuracy: 0.1582 - val_loss: 2.1761\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1238 - loss: 2.2736 - val_accuracy: 0.1060 - val_loss: 2.3027\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1076 - loss: 2.3026 - val_accuracy: 0.1060 - val_loss: 2.3029\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1098 - loss: 2.3025 - val_accuracy: 0.1060 - val_loss: 2.3032\n",
      "\n",
      "\n",
      "\n",
      "Learning rate:  0.03\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.1272 - loss: 2.6074 - val_accuracy: 0.1060 - val_loss: 2.3053\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1071 - loss: 2.3044 - val_accuracy: 0.1060 - val_loss: 2.3066\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1064 - loss: 2.3046 - val_accuracy: 0.1126 - val_loss: 2.3055\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1086 - loss: 2.3041 - val_accuracy: 0.1126 - val_loss: 2.3033\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1063 - loss: 2.3048 - val_accuracy: 0.0988 - val_loss: 2.3030\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1077 - loss: 2.3053 - val_accuracy: 0.1060 - val_loss: 2.3045\n",
      "Epoch 7/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1063 - loss: 2.3051 - val_accuracy: 0.1060 - val_loss: 2.3066\n",
      "Epoch 8/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1087 - loss: 2.3045 - val_accuracy: 0.0964 - val_loss: 2.3048\n",
      "Epoch 9/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1041 - loss: 2.3053 - val_accuracy: 0.1060 - val_loss: 2.3062\n",
      "Epoch 10/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1062 - loss: 2.3054 - val_accuracy: 0.1060 - val_loss: 2.3070\n",
      "\n",
      "\n",
      "\n",
      "Learning rate:  0.05\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1025 - loss: 2.3077 - val_accuracy: 0.1060 - val_loss: 2.3088\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1058 - loss: 2.3074 - val_accuracy: 0.1126 - val_loss: 2.3055\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1054 - loss: 2.3081 - val_accuracy: 0.0986 - val_loss: 2.3048\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1037 - loss: 2.3083 - val_accuracy: 0.1060 - val_loss: 2.3118\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1036 - loss: 2.3078 - val_accuracy: 0.1060 - val_loss: 2.3153\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1045 - loss: 2.3076 - val_accuracy: 0.0998 - val_loss: 2.3084\n",
      "Epoch 7/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1033 - loss: 2.3082 - val_accuracy: 0.1018 - val_loss: 2.3102\n",
      "Epoch 8/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1053 - loss: 2.3080 - val_accuracy: 0.1060 - val_loss: 2.3109\n",
      "\n",
      "\n",
      "\n",
      "Learning rate:  0.05\n",
      "Epoch 1/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.1086 - loss: 2.3075 - val_accuracy: 0.1060 - val_loss: 2.3043\n",
      "Epoch 2/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1053 - loss: 2.3074 - val_accuracy: 0.1060 - val_loss: 2.3043\n",
      "Epoch 3/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1058 - loss: 2.3074 - val_accuracy: 0.0988 - val_loss: 2.3087\n",
      "Epoch 4/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1015 - loss: 2.3075 - val_accuracy: 0.1126 - val_loss: 2.3079\n",
      "Epoch 5/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1031 - loss: 2.3078 - val_accuracy: 0.1060 - val_loss: 2.3077\n",
      "Epoch 6/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1028 - loss: 2.3079 - val_accuracy: 0.1060 - val_loss: 2.3087\n",
      "Epoch 7/70\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1063 - loss: 2.3074 - val_accuracy: 0.0978 - val_loss: 2.3122\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creamos carpeta si no existe para guardar los modelos y checkpoint\n",
    "ruta_dir_mod = './recursos/modelos/4_3_2'\n",
    "if not os.path.exists(ruta_dir_mod):\n",
    "    os.mkdir(ruta_dir_mod)\n",
    "if not os.path.exists('./recursos/checkpoint'):\n",
    "    os.mkdir('./recursos/checkpoint')\n",
    "\n",
    "# Si no mejora en 7 iteraciones el modelo para de entrenar\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./recursos/checkpoint/4_3_2_checkpoints.weights.h5\", save_weights_only=True)\n",
    "\n",
    "historys = []\n",
    "rangos = [1e-3, 3e-3, 5e-3, 1e-2, 3e-2, 5e-2, 0.05]\n",
    "for lr in rangos :\n",
    "    print('Learning rate: ',lr)\n",
    "    # Se compila el modelo con el optimizador   \n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=lr),   # asignar tasa de aprendizaje\n",
    "              metrics=['accuracy'])\n",
    "    # entrenar\n",
    "    hist = model.fit(X_train, y_train, epochs=70, validation_data=(X_valid, y_valid), \n",
    "                     callbacks=[checkpoint_cb,early_stopping_cb])\n",
    "    historys.append(hist)\n",
    "    \n",
    "    # guardar\n",
    "    ruta = \"./recursos/modelos/4_3_2/my_keras_model_lr{}.keras\".format(lr)\n",
    "    model.save(ruta)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo_0: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n",
      " Modelo_1: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n",
      " Modelo_2: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n",
      " Modelo_3: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n",
      " Modelo_4: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n",
      " Modelo_5: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n",
      " Modelo_6: {'verbose': 'auto', 'epochs': 70, 'steps': 1719}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(historys)):\n",
    "    # mostramos el historial de entrenamiento \n",
    "    print(' Modelo_{}: {}'.format(i,historys[i].params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar y evaluar con bucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluacion my_keras_model_lr1.0.keras: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_4_1/Cast:0\", shape=(None, 32, 32, 3), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (None, 32, 32, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 32, 32, 3), dtype=uint8)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# evaluar\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluacion \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mod_ruta))\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_4_1/Cast:0\", shape=(None, 32, 32, 3), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (None, 32, 32, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 32, 32, 3), dtype=uint8)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# cargar\n",
    "modelos = os.listdir(ruta_dir_mod)\n",
    "\n",
    "for mod_ruta in modelos:\n",
    "    model = tf.keras.models.load_model(ruta_dir_mod+'/'+mod_ruta)\n",
    "    # evaluar\n",
    "    print('Evaluacion {}: '.format(mod_ruta))\n",
    "    eval = model.evaluate(X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificación de Imágenes de Dígitos Escritos a Mano:\n",
    "\n",
    "* Objetivo: Entrenar una red neuronal para clasificar imágenes de dígitos escritos a mano utilizando el conjunto de datos SVHN (Street View House Numbers).\n",
    "\n",
    "* Tareas: \n",
    "    * Cargar el conjunto de datos, dividir los datos (train, valid y test), explorar los datos representando alguna imagen con su etiqueta y muestra los valores (únicos) de y (en el entrenamiento, en validación y en pruebas), preparar los datos (d normalizar dividiendo entre 255.0 e igual hay que hacer algo más), construir, compilar y entrenar una red neuronal. \n",
    "\n",
    "    * Utilizar detección temprana\n",
    "\n",
    "    * Hacer una gráfica de como evoluciona accuracy y val_accuracy durante el entrenamiento\n",
    "\n",
    "    * Si es necesario ajustar el modelo para lograr una mayor precisión.\n",
    "\n",
    "    * Evalúar el modelo en el conjunto de test y hacer alguna predicción (representa alguna imagen con su etiqueta real y la predicha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_svhn_data(url):\n",
    "    path = tf.keras.utils.get_file(\"svhn.mat\", url)\n",
    "    data = loadmat(path)\n",
    "    return np.transpose(data[\"X\"], (3, 0, 1, 2)), data[\"y\"].flatten()\n",
    "\n",
    "# Cargar datos\n",
    "train_url = \"http://ufldl.stanford.edu/housenumbers/train_32x32.mat\"\n",
    "test_url = \"http://ufldl.stanford.edu/housenumbers/test_32x32.mat\"\n",
    "X_train_full, y_train_full = load_svhn_data(train_url)\n",
    "# entrenamiento y validacion\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full)\n",
    "# prueba\n",
    "X_test, y_test = load_svhn_data(test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  62268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[103, 110, 125],\n",
       "        [103, 110, 126],\n",
       "        [105, 112, 128],\n",
       "        ...,\n",
       "        [ 71,  80,  98],\n",
       "        [ 66,  78,  95],\n",
       "        [ 63,  76,  93]],\n",
       "\n",
       "       [[104, 112, 126],\n",
       "        [105, 112, 127],\n",
       "        [104, 113, 128],\n",
       "        ...,\n",
       "        [ 53,  61,  86],\n",
       "        [ 43,  52,  76],\n",
       "        [ 38,  47,  71]],\n",
       "\n",
       "       [[105, 115, 127],\n",
       "        [105, 116, 128],\n",
       "        [105, 115, 127],\n",
       "        ...,\n",
       "        [ 39,  43,  73],\n",
       "        [ 32,  36,  65],\n",
       "        [ 31,  36,  65]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[105, 111, 127],\n",
       "        [103, 110, 126],\n",
       "        [104, 110, 128],\n",
       "        ...,\n",
       "        [ 93,  96, 117],\n",
       "        [102, 106, 125],\n",
       "        [107, 111, 128]],\n",
       "\n",
       "       [[105, 111, 127],\n",
       "        [103, 109, 125],\n",
       "        [106, 112, 128],\n",
       "        ...,\n",
       "        [103, 106, 126],\n",
       "        [105, 110, 128],\n",
       "        [108, 113, 132]],\n",
       "\n",
       "       [[106, 112, 128],\n",
       "        [105, 111, 127],\n",
       "        [107, 113, 130],\n",
       "        ...,\n",
       "        [106, 109, 130],\n",
       "        [107, 112, 131],\n",
       "        [108, 113, 132]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('len: ',len(X_train))\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción del consumo de energía:\n",
    "\n",
    "* Objetivo: Construir una red neuronal para predecir el consumo de energía de edificios a partir de características como la temperatura, humedad, y tiempo del conjunto de datos Energy Efficiency de UCI Machine Learning Repository\n",
    "\n",
    "* Tareas: \n",
    "    * Cargar el conjunto de datos, dividir los datos (train, valid y test), explorar los datos, preparar los datos (normalizar con una capa de normalización), construir un modelo de regresión, compilar y entrenar una red neuronal. \n",
    "    \n",
    "    * Usa X1-X8 para la X e Y1-Y2 para la y\n",
    "\n",
    "    * Ajusta el modelo hasta encontrar un error absoluto medio aceptable\n",
    "\n",
    "    * Haz una gráfica de como evoluciona el error absoluto medio y error absoluto medio del conjunto de validación durante el entrenamiento\n",
    "\n",
    "    * Evalúa el modelo en el conjunto de test y haz alguna predicción (muestra el valor predicho y el real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el conjunto de datos de eficiencia energética de UCI\n",
    "url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\")\n",
    "data = pd.read_excel(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
