<p dir="ltr" style="text-align:left;"></p>
<h4>Ingesta de Datos</h4>
<p dir="ltr">La ingesta de datos es el proceso de importar grandes archivos de datos de múltiples fuentes a un único sistema de almacenamiento basado en la nube —un data warehouse, data mart o base de datos—desde el que se puede acceder a los mismos y analizarlos. Como los datos tienen diferentes formas y proceden de centenares de fuentes, se limpian y transforman en un formato único utilizando un proceso de extraer/transformar/cargar (ETL).</p>
<p dir="ltr">Las formas más comunes para recopilar datos son: formularios, encuestas, entrevistas y observaciones directas.</p><br>
<h4>Transformación de Datos</h4>
<p dir="ltr">La transformación de datos se refiere a la conversión y optimización de datos para diversos propósitos, como los análisis, la elaboración de informes o el almacenamiento. Implica limpiar, estructurar y enriquecer datos para garantizar su precisión y relevancia. A menudo, las soluciones de transformación de datos utilizan tecnologías avanzadas, como IA (inteligencia artificial) y ML (Machine Learning), para optimizar y automatizar estos procesos.</p>
<p dir="ltr">La transformación de datos puede incluir varias de las siguientes operaciones:</p>
<ul>
    <li dir="ltr">
        <p dir="ltr">Limpieza de datos: Eliminar errores, incoherencias y valores perdidos para garantizar datos fiables de alta calidad.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Estandarización: Escalar datos numéricos para obtener una media de 0 y una desviación estándar de 1 y que resulten compatibles con determinados algoritmos.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Codificar datos categóricos: Convertir variables categóricas en formatos numéricos para el procesamiento algorítmico.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Agregación: resumir datos calculando medias, sumas o recuentos en categorías o intervalos de tiempo específicos.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Ingeniería de características: Crear nuevos atributos de datos a partir de los existentes para capturar información o relaciones adicionales.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Reducción de datos: Reducir la dimensionalidad de los datos seleccionando características relevantes o utilizando técnicas como el análisis de componentes principales (PCA).</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Descomposición de series temporales: Descomponer los datos de series temporales en componentes de tendencias, estacionalidad y ruido para analizarlos de forma independiente.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Binning o discretización: Agrupar datos continuos en categorías diferenciadas, algo que resulta de gran utilidad para gestionar datos ruidosos.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Suavizado: Aplicar métodos como el de las medias en movimiento para reducir el ruido en la serie temporal o crear datos suavizados.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Transformación logarítmica o exponencial: Alterar la distribución de datos mediante funciones logarítmicas o exponenciales para análisis especializados.</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Preprocesamiento de texto: Preparar datos de texto para tareas de procesamiento de lenguaje natural (NLP) mediante tokenización, stemming o lematización.</p>
    </li>
</ul><br>
<h4>Almacenamiento de Datos</h4>
<p dir="ltr">El almacenamiento y gestión de los macrodatos implica una serie de técnicas y tecnologías que permite manejar enormes volúmenes de información. Esto marca una diferencia con el almacenamiento de datos tradicional, que suele estar limitado por la capacidad de discos duros o por la arquitectura de bases de datos relacionales, y cuenta con los siguientes elementos clave:</p>
<ul>
    <li dir="ltr">
        <p dir="ltr">Escalabilidad:</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Redundancia y Replicación:</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Almacenamiento Distribuido:</p>
    </li>
</ul>
<p dir="ltr">Para el control de estos existen distintos tipos de almacenamiento enfocados a&nbsp; la gestión de la información y los grandes conjuntos de datos. Los principales son:</p>
<ul>
    <li dir="ltr">
        <p dir="ltr">Almacenamiento en la Nube</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Hadoop Distributed File System (HDFS)</p>
    </li>
    <li dir="ltr">
        <p dir="ltr">Bases de Datos NoSQL</p>
    </li>
</ul>
<p><br></p>
<h4>Procesamiento de Datos</h4>
<p dir="ltr">Los profesionales del Big Data deben organizar, almacenar y recuperar datos según sea necesario durante todo el ciclo de un proyecto como parte del procesamiento de los datos, por ello se trata de un proceso de naturaleza continua, tomando lugar desde el comienzo de un proyecto hasta el final.</p>
<h4>Análisis de Datos</h4>
<p dir="ltr">Esta es la fase de Big Data clave. Una vez procesados, almacenados y hecha la gestión de las bases de datos, llega el momento de analizarlos.</p>
<p dir="ltr">No obstante, el análisis de Big Data se puede hacer sobre datos no procesados. Para ello, los analistas emplean diferentes herramientas y estrategias como, por ejemplo: modelado estadístico, algoritmos, inteligencia artificial, minería de datos, aprendizaje automático.</p>
<p dir="ltr">Cada una de estas estrategias es válida para un tipo de escenario específico.</p><br>
<h4>Visualización de Datos</h4>
<p dir="ltr">Esta fase se refiere al proceso de creación de representaciones gráficas de información, generalmente mediante el uso de una o más herramientas de visualización.</p>
<p dir="ltr">Gracias a esto, la posterior interpretación del análisis Big Data es más sencillo. Y es que, la visualización facilita la comunicación rápida de su análisis a una audiencia más amplia.</p>
<div><br></div>
<div>
    <h4>Seguridad de Datos</h4>
    <p dir="ltr">Para poder asegurar debidamente los datos tendremos que asegurarnos de que contamos con una buena gobernabilidad sobre los mismos. Esto significa que los datos estén previamente autorizados (control de ingesta), organizados (estructuración de los datos), con menor número posible de errores y redundancias, manteniendo la privacidad y la seguridad de los mismos.</p>
    <ul>
        <li dir="ltr">
            <p dir="ltr">Acceso y autorización granular a datos:</p>
        </li>
    </ul>
    <p dir="ltr">Uso de expresiones de control de acceso. Estas expresiones usan agrupación y lógica booleana para controlar el acceso y autorización de datos flexibles, con permisos basados en roles y configuraciones de visibilidad.</p>
    <p dir="ltr">En el nivel más bajo, se protegen los datos confidenciales, ocultándolos, y en la parte superior, se tienen contratos confidenciales para científicos de datos y analistas de BD.</p>
    <ul>
        <li dir="ltr">
            <p dir="ltr">Seguridad perimetral, protección de datos y autenticación integrada</p>
        </li>
    </ul>
    <p dir="ltr">Es importante construir un buen perímetro alrededor de los datos, integrados con los sistemas y estándares de autenticación existentes. <br></p>
    <p dir="ltr">La autenticación consiste en integrar nuestro sistema con opciones ya establecidas como pueden ser, LDAP, Active Directory u otros servicios de directorio. También se puede dar soporte a herramientas como Kerberos para soporte de autenticación.</p>
    <ul>
        <li dir="ltr">
            <p dir="ltr">Encriptación y Tokenización de Datos</p>
        </li>
    </ul>
    <p dir="ltr">Almacenando los datos codificados impedimos el acceso exterior a los mismos, blindando nuestros sistemas ante posibles filtraciones o accesos no autorizados.</p>
    <ul>
        <li dir="ltr">
            <p dir="ltr">Constante Auditoría y Análisis</p>
        </li>
        <li dir="ltr">
            <p dir="ltr">Arquitectura de Datos Unificada</p>
        </li>
    </ul><br>
    <h4>Monitorización de Datos</h4>
    <p dir="ltr">La correcta monitorización de los macrodatos garantiza la seguridad, confiabilidad y accesibilidad de los datos almacenados. Con herramientas que permiten monitorizar la calidad de los datos, llevar una precisa trazabilidad sobre el origen de los mismos y gestionar los metadatos generados.</p>
    <p dir="ltr">Una correcta monitorización permite a su vez identificar anomalías e inconsistencias en los datos, garantizando la precisión e integridad de los mismos. Garantizando a las entidades que la información que cimienta sus decisiones es correcta.</p><br>
</div>
<div><br></div>
<div>[[Datos]]</div>
<div>[[¿Qué es BIG DATA?]]<br></div>
<p></p>
